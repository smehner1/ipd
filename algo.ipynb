{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingress Detection Algorithm\n",
    "\n",
    "```\n",
    "cidr_max = 28   # nax split cidr mask\n",
    "t=60            # seconds\n",
    "e=120           # expire time if IPs\n",
    "q = 0.95        # needed ingress fraction\n",
    "c = 64          # c* sqrt(2^(IPv * max^-cidr)) -> min number of sampled\n",
    "\n",
    "start with no knowledge (only /0 is known) \n",
    "loop\n",
    "    collect IP from Netflow\n",
    "        Filter IPs for ingress\n",
    "        Mask them to cidrmax\n",
    "        Insert IP into corresponding prefix_range\n",
    "\n",
    "Every t seconds \n",
    "    Check all ranges\n",
    "        Remove IPs older than e seconds \n",
    "        Prevalent color still valid (s_color >= q)\n",
    "            YES → join siblings ? (join(s_color ) >= q) \n",
    "                YES → join siblings and check again \n",
    "                NO → do nothing\n",
    "            NO → remove all information\n",
    "\n",
    "        Check if enough samples have been collected (s_ipcount >= n_cidr ) \n",
    "            YES → is a single color prevalent ? (s_color >=q)\n",
    "                YES → color range with link color\n",
    "                NO → split subnet if s_cidr < cidrmax\n",
    "            NO → join siblings ? (join(s_color) >= q or join(s_ipcount) < n_cidr−1)\n",
    "                YES → join siblings and check again \n",
    "                NO → do nothing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_338462/1146163602.py:86: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  netflow_df = pd.read_csv(netflow_path, compression='gzip', header=None, sep=',', quotechar='\"', error_bad_lines=False, names=cols, usecols = ['peer_src_ip', 'in_iface', 'src_ip', 'ts_end'])\n",
      "/tmp/ipykernel_338462/1146163602.py:86: DtypeWarning: Columns (2,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  netflow_df = pd.read_csv(netflow_path, compression='gzip', header=None, sep=',', quotechar='\"', error_bad_lines=False, names=cols, usecols = ['peer_src_ip', 'in_iface', 'src_ip', 'ts_end'])\n",
      "/tmp/ipykernel_338462/1146163602.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  netflow_df.drop(columns=['is_ingresslink'], inplace=True)\n",
      "/tmp/ipykernel_338462/1146163602.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  netflow_df['ts_end'] = netflow_df.ts_end.apply(lambda x: int(int(x) / t) * t)\n",
      "/tmp/ipykernel_338462/1146163602.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  netflow_df.sort_values(by = 'ts_end', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_ip</th>\n",
       "      <th>ts_end</th>\n",
       "      <th>ingress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86492</th>\n",
       "      <td>31.13.84.52</td>\n",
       "      <td>1605639480</td>\n",
       "      <td>VIE-SB5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82140</th>\n",
       "      <td>81.182.112.33</td>\n",
       "      <td>1605639480</td>\n",
       "      <td>VIE-SB5.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82141</th>\n",
       "      <td>45.57.17.147</td>\n",
       "      <td>1605639480</td>\n",
       "      <td>VIE-SB5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89876</th>\n",
       "      <td>52.46.159.143</td>\n",
       "      <td>1605639480</td>\n",
       "      <td>VIE-SB5.1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89874</th>\n",
       "      <td>23.47.213.80</td>\n",
       "      <td>1605639480</td>\n",
       "      <td>VIE-SB5.1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35676</th>\n",
       "      <td>31.13.84.15</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35677</th>\n",
       "      <td>31.13.84.15</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35678</th>\n",
       "      <td>31.13.84.15</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35722</th>\n",
       "      <td>31.13.84.4</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>145.236.122.145</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66949 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                src_ip      ts_end       ingress\n",
       "86492      31.13.84.52  1605639480    VIE-SB5.25\n",
       "82140    81.182.112.33  1605639480    VIE-SB5.17\n",
       "82141     45.57.17.147  1605639480    VIE-SB5.18\n",
       "89876    52.46.159.143  1605639480  VIE-SB5.1507\n",
       "89874     23.47.213.80  1605639480  VIE-SB5.1493\n",
       "...                ...         ...           ...\n",
       "35676      31.13.84.15  1605639600     VIE-SB5.9\n",
       "35677      31.13.84.15  1605639600     VIE-SB5.9\n",
       "35678      31.13.84.15  1605639600     VIE-SB5.9\n",
       "35722       31.13.84.4  1605639600     VIE-SB5.9\n",
       "99999  145.236.122.145  1605639600    VIE-SB5.17\n",
       "\n",
       "[66949 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install netaddr\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import gzip\n",
    "import pytricia\n",
    "import ipaddress\n",
    "from netaddr import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import dpath.util as dp\n",
    "import io\n",
    "import os\n",
    "import logging\n",
    "\n",
    "loglev= logging.INFO # DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "t = 60          # seconds\n",
    "bucket_output = 60\n",
    "e=  120         # 120  # expire time if IPs\n",
    "q = 0.7801        # needed ingress fraction\n",
    "b= 0.05         # allowed delta between bundle load\n",
    "\n",
    "\n",
    "cidr_max = {    # nax split cidr mask\n",
    "    4: 28,\n",
    "    6: 48\n",
    "}\n",
    "c ={            # c* sqrt(2^(IPv * max^-cidr))\n",
    "    4: 0.0064,\n",
    "    6: 0.0024\n",
    "}\n",
    "decay_method='default'\n",
    "\n",
    "decay_ingmar_bucket_expire_keep_fraction=0.9\n",
    "\n",
    "cols=['tag', 'peer_src_ip', 'in_iface', 'out_iface', 'src_ip', 'dst_net', 'src_port', 'dst_port', 'proto', '__', '_', 'ts_start', 'ts_end', 'pkts', 'bytes']\n",
    "bundle_indicator=\".b_\"\n",
    "\n",
    "netflow_path=\"/data/slow/mehner/netflow/dummy_netflow.gz\"\n",
    "ingresslink_file = \"/data/slow/mehner/ingresslink/1605571200.gz\"                # if we get more netflow, we should adjust the file\n",
    "router_ip_mapping_file=\"/data/slow/mehner/router_lookup_tables/1605571200.txt\"\n",
    "\n",
    "\n",
    "############################################\n",
    "########### LOGGER CONFIGURATION ###########\n",
    "############################################\n",
    "os.makedirs(\"log\", exist_ok=True)\n",
    "logfile=f\"log/q{q}_c{c[4]}-{c[6]}_cidr_max{cidr_max[4]}-{cidr_max[6]}_t{t}_e{e}_decay{decay_method}.log\"\n",
    "logging.basicConfig(filename=logfile,\n",
    "                    format='%(asctime)s %(levelname)s %(funcName)s %(message)s',\n",
    "                    datefmt='%d-%b-%y %H:%M:%S',\n",
    "                    filemode='w',\n",
    "                    level=loglev)\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "###################################################\n",
    "########### ROUTER NAME <--> IP MAPPING ###########\n",
    "###################################################\n",
    "with open(router_ip_mapping_file, 'r') as csv_file:\n",
    "    router_ip_mapping_csv = csv.reader(csv_file, delimiter=' ')\n",
    "    router_ip_lookup_dict = {rows[0]:rows[1] for rows in router_ip_mapping_csv}\n",
    "\n",
    "###################################################\n",
    "###########     INGRESS LINK FILE       ###########\n",
    "###################################################\n",
    "\n",
    "logger.info(f\"load ingresslink file: {ingresslink_file}\")\n",
    "ingresslink_dict= {}\n",
    "with gzip.open(\"{}\".format(ingresslink_file), 'rb') as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8').split(\",\")\n",
    "        router= line[0].replace(\"PEER_SRC_IP=\", \"\")\n",
    "        in_iface= line[1].replace(\"IN_IFACE=\", \"\")\n",
    "\n",
    "        # ingresslink_list.append(\"{}.{}\".format(router, in_iface))\n",
    "        ingresslink_dict[\"{}.{}\".format(router, in_iface)] = True\n",
    "logger.info(\"  ...done\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# TAG     PEER_SRC_IP  IN IFACE OUT_IFACE SRC_IP          DST_NET        SRC_PORT DST_PORT PROTO  _       _       TS_START        TS_END    PKTS    BYTES\n",
    "# 0       194.25.7.141    13      1571    91.127.69.122   31.13.84.4      40730   443     tcp     0       i       1605639641      1605639641 1       121\n",
    "netflow_df = pd.read_csv(netflow_path, compression='gzip', header=None, sep=',', quotechar='\"', error_bad_lines=False, names=cols, usecols = ['peer_src_ip', 'in_iface', 'src_ip', 'ts_end'])\n",
    "logger.debug(\"read: {}\".format(len(netflow_df)))\n",
    "\n",
    "\n",
    "## pandas pipe  -> https://towardsdatascience.com/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0\n",
    "netflow_df['ingress_router'] = netflow_df.peer_src_ip.apply(lambda x: router_ip_lookup_dict.get(x))\n",
    "netflow_df['ingress'] = netflow_df['ingress_router'] + \".\" + netflow_df.in_iface.astype(str)\n",
    "netflow_df.drop(columns=['ingress_router', 'peer_src_ip', 'in_iface'], inplace=True)\n",
    "\n",
    "netflow_df.drop(netflow_df.index[netflow_df['ts_end'] == 'TIMESTAMP_END'], inplace=True)\n",
    "\n",
    "netflow_df['is_ingresslink'] = netflow_df.ingress.apply(lambda x: ingresslink_dict.get(x,False))\n",
    "netflow_df = netflow_df.loc[netflow_df.is_ingresslink]\n",
    "\n",
    "netflow_df.drop(columns=['is_ingresslink'], inplace=True)\n",
    "logger.debug(\"ingress only: \", len(netflow_df))\n",
    "\n",
    "netflow_df['ts_end'] = netflow_df.ts_end.apply(lambda x: int(int(x) / t) * t)\n",
    "netflow_df.sort_values(by = 'ts_end', inplace=True)\n",
    "\n",
    "# mask to cidr max\n",
    "#netflow_df['src_ip'] = netflow_df.src_ip.apply(lambda x: str(ipaddress.ip_network(\"{}/{}\".format(x, cidr_max), strict=False)).split(\"/\")[0])\n",
    "\n",
    "netflow_df = netflow_df.convert_dtypes()\n",
    "\n",
    "netflow_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netflow_df= pd.read_csv(\"/data/slow/mehner/netflow100000.csv\", names= [\"src_ip\",\"ts_end\",\"ingress\"])\n",
    "# netflow_df['ts_end'] = netflow_df.ts_end.apply(lambda x: int(int(x) / t) * t) \n",
    "# netflow_df.sort_values(by = 'ts_end', inplace=True)\n",
    "# netflow_df.head()\n",
    "# netflow_df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################\n",
    "### PROTOTYPING IPDRange Class ###\n",
    "##################################\n",
    "\n",
    "### DICT implementation\n",
    "\n",
    "# if classified range will be in range dict\n",
    "# def __range_atts():\n",
    "    # NOTE last seen will be updated if there is any new IP that belongs to this range\n",
    "    #   if last_seen < 'current now' - e: drop prefix\n",
    "    # return {'last_seen': 0, 'ingress': \"\", 'match' : 0, 'miss' : 0}\n",
    "\n",
    "# if not yet classified range will be in subnet dict - here ip addresses are monitored\n",
    "def __subnet_atts():\n",
    "    return {'last_seen': 0,  'ingress' : \"\"}\n",
    "\n",
    "def __multi_dict(K, type):\n",
    "    if K == 1:\n",
    "        return defaultdict(type)\n",
    "    else:\n",
    "        return defaultdict(lambda: __multi_dict(K-1, type))\n",
    "\n",
    "# something like range_dict[ip_version][range]{last_seen: ... , ingress: ... , match: ... , miss: ... }\n",
    "# range_dict=__multi_dict(2, __range_atts)\n",
    "\n",
    "# something like subnet_dict[ip_version][range][{ip: ... , ingress: ... , last_seen: ... }]\n",
    "#subnet_dict=__multi_dict(3, __subnet_atts)\n",
    "subnet_dict=__multi_dict(4, __subnet_atts) # smehner TESTING\n",
    "\n",
    "# initialization\n",
    "range_lookup_dict = __multi_dict(1, pytricia.PyTricia) #defaultdict(lambda: pytricia.PyTricia())\n",
    "range_lookup_dict[4].insert(\"0.0.0.0/0\", \"0.0.0.0/0\")\n",
    "range_lookup_dict[6].insert(\"::/0\", \"::/0\")\n",
    "\n",
    "bundle_dict={}\n",
    "\n",
    "## lookup in pytricia tree and return corresponding range\n",
    "def get_corresponding_range(ip):\n",
    "    ip_version = 4 if not \":\" in ip else 6\n",
    "    try:\n",
    "        res =range_lookup_dict[ip_version][ip]\n",
    "    except:\n",
    "        logger.warning(f\"key error: {ip}\")\n",
    "        logger.debug(\"  current ranges: {}\".format(list(range_lookup_dict[ip_version])))\n",
    "\n",
    "        res=\"0.0.0.0/0\" if ip_version == 4 else \"::/0\"\n",
    "    # logger.info(\"check corresponding range;  ip: {} ; range: {}\".format(ip_address, res))\n",
    "    return res\n",
    "\n",
    "def mask_ip(ip_address):\n",
    "    ip_version = 6 if \":\" in ip_address else 4\n",
    "    return str(IPNetwork(f\"{ip_address}/{cidr_max[ip_version]}\").network)\n",
    "\n",
    "\n",
    "\n",
    "def __get_min_samples(path, decrement=False):\n",
    "        t = path.split(\"/\")\n",
    "        ip_version = int(t[0])\n",
    "        cidr = int(t[1])\n",
    "\n",
    "        if decrement:\n",
    "            cc= c[ip_version] * 0.001 # take 1% of min_samples as decrement base\n",
    "        else:\n",
    "            cc = c[ip_version]\n",
    "\n",
    "\n",
    "        ipv_max = 32\n",
    "        if ip_version == 6:\n",
    "            ipv_max = 64\n",
    "        min_samples=int(cc * math.sqrt( math.pow(2, (ipv_max - cidr))))\n",
    "\n",
    "        # logger.info(f\"min samples: {min_samples}\")\n",
    "        return min_samples\n",
    "\n",
    "def __split_ip_and_mask(prefix):\n",
    "    # prefix should be in this format 123.123.123.123/12 or 2001:db8:abcd:0012::0/64\n",
    "\n",
    "    ip = prefix.split(\"/\")[0]\n",
    "    mask = prefix.split(\"/\")[1]\n",
    "\n",
    "    return str(ip), int(mask)\n",
    "\n",
    "def __convert_range_string_to_range_path(range_string):\n",
    "    ip_version = 4 if not \":\" in range_string else 6\n",
    "\n",
    "    prange, mask = range_string.split(\"/\")\n",
    "\n",
    "    return f\"{ip_version}/{mask}/{prange}\"\n",
    "\n",
    "def __convert_range_path_to_single_elems(path):\n",
    "    t = path.split(\"/\")\n",
    "    ip_version = int(t[0])\n",
    "    mask = int(t[1])\n",
    "    prange= t[2]\n",
    "    return ip_version, mask, prange\n",
    "\n",
    "def __sort_dict(dict_to_sort):\n",
    "    return {k: dict_to_sort[k] for k in sorted(dict_to_sort, key=dict_to_sort.__getitem__, reverse=True)}\n",
    "\n",
    "\n",
    "def get_sample_count(path):\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "    logger.debug(path)\n",
    "    # matc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}).get('match', -1)\n",
    "    count = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}).get('total', -1)\n",
    "    # misc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}).get('miss', -1)\n",
    "\n",
    "    if count < 0:\n",
    "\n",
    "        # if no prevalent ingress exists, count all items\n",
    "        count= len(subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}))\n",
    "\n",
    "        if count <=0:\n",
    "            logger.warning(f\" key {path} does not exist\")\n",
    "            return -1\n",
    "\n",
    "    return count\n",
    "\n",
    "def check_if_enough_samples_have_been_collected(prange):\n",
    "    logger.info(f\"  > Check if enough samples have been collected (s_ipcount >= n_cidr ) {prange}\")\n",
    "    sample_count = get_sample_count(prange)\n",
    "    if sample_count < 0: # if -1 -> key error\n",
    "        return None\n",
    "\n",
    "    min_samples=__get_min_samples(prange)\n",
    "    logger.debug(f\"sample_count: {sample_count} || min_samples= {min_samples}\")\n",
    "\n",
    "    if sample_count >= min_samples:\n",
    "        # print(\"    YES → is a single color prevalent ? (s_color >=q)\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# if raw=True: return not prevalent ingress, but dict with counters for all found routers\n",
    "def get_prevalent_ingress(path, raw=False):\n",
    "\n",
    "    cur_prevalent=None\n",
    "    ratio= -1\n",
    "    sample_count=get_sample_count(path)\n",
    "\n",
    "    # calculate prevalent ingress\n",
    "    counter_dict=defaultdict(int)\n",
    "    result_dict={}\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "    p_ingress = subnet_dict.get(ip_version, {}).get(mask, {}).get(prange, {}).get('prevalent', None)\n",
    "    p_total = subnet_dict.get(ip_version, {}).get(mask, {}).get(prange, {}).get('total', None)\n",
    "    p_miss = subnet_dict.get(ip_version, {}).get(mask, {}).get(prange, {}).get('miss', None)\n",
    "\n",
    "    if p_ingress != None and p_total != None: # there is a prevalent ingress yet\n",
    "        if p_total < 1: \n",
    "            pr = subnet_dict[ip_version][mask].pop(prange)\n",
    "            logger.warning(f\"p_total < 1: {path} ingress:{p_ingress} total:{p_total} miss:{p_miss} - pop: {pr}\")\n",
    "            \n",
    "            return None\n",
    "\n",
    "        ratio = 1- (p_miss / p_total)\n",
    "\n",
    "        if raw:\n",
    "            return {p_ingress : (p_total-p_miss), 'miss' : p_miss} # TODO total or matches ?\n",
    "\n",
    "        if ratio >= q:\n",
    "            logger.debug(f\"        already classified: {p_ingress}: ({ratio:.2f})\")\n",
    "            cur_prevalent=p_ingress\n",
    "        else:\n",
    "            logger.warning(f\"        prevalent ingress {p_ingress} for {path} below threshold ({ratio})\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        search_path=\"{}/**/ingress\".format(path)\n",
    "        for p, v in dp.search(subnet_dict, search_path, yielded=True):\n",
    "            counter_dict[v]+=1\n",
    "\n",
    "        # is single ingress prevalent?\n",
    "\n",
    "        for ingress in counter_dict:\n",
    "            ratio = counter_dict.get(ingress) / sample_count\n",
    "            result_dict[ingress] = round(ratio,3)\n",
    "\n",
    "            # logger.info(\"       ratio for {}: {:.2f}\".format(ingress, ratio))\n",
    "            if ratio >= q:  # we found a prevalent ingress point!\n",
    "                cur_prevalent = ingress\n",
    "                if not raw: break\n",
    "\n",
    "        # check for bundles\n",
    "        if cur_prevalent == None: # if we still have not found an ingress, maybe we have a bundle here\n",
    "            bundle_candidates=set()\n",
    "            last_value=None\n",
    "            last_ingress=None\n",
    "            logger.debug(__sort_dict(result_dict))\n",
    "            for ingress in __sort_dict(result_dict):\n",
    "                value = result_dict.get(ingress)\n",
    "                if value < 0.1: break # since it is sorted; otherwise we should use continue here\n",
    "\n",
    "                # first iteration\n",
    "                if last_value == None:\n",
    "                    last_value = value\n",
    "                    last_ingress = ingress\n",
    "                    continue\n",
    "\n",
    "                # 2nd ... nth iteration\n",
    "                if value + b >= last_value:\n",
    "                        # check if there is the same router\n",
    "                        if ingress.split(\".\")[0] == last_ingress.split(\".\")[0]:\n",
    "                            bundle_candidates.add(last_ingress)\n",
    "                            bundle_candidates.add(ingress)\n",
    "\n",
    "                last_value = value\n",
    "                last_ingress = ingress\n",
    "\n",
    "            if len(bundle_candidates) > 0:\n",
    "                logger.debug(f\"bundle candidates: {bundle_candidates}\")\n",
    "                cum_ratio=0\n",
    "                for i in bundle_candidates: cum_ratio += result_dict.get(i)\n",
    "\n",
    "                if cum_ratio >= q:\n",
    "                    # if cum_ratio exceeds q, this will be a bundle\n",
    "                    cur_prevalent=list(bundle_candidates)\n",
    "                    ratio = cum_ratio\n",
    "\n",
    "\n",
    "        if raw:\n",
    "            logger.debug(f\"counter_dict: {counter_dict}\")\n",
    "            return counter_dict\n",
    "\n",
    "\n",
    "    if cur_prevalent == None:\n",
    "        ratio = -1\n",
    "        logger.info(\"        no prevalent ingress found: {}\".format(__sort_dict(result_dict)))\n",
    "\n",
    "    logger.info(\"        prevalent for {}: {} ({:.2f})\".format(path, cur_prevalent, ratio))\n",
    "\n",
    "    return cur_prevalent\n",
    "\n",
    "def set_prevalent_ingress(path, ingress):\n",
    "    # if an ingress is prevalent we set a 'prevalent' attribute for this path\n",
    "    # then we can set the counter for miss and match\n",
    "    # and pop the list with all single ips\n",
    "    # then we need to distinguish between\n",
    "    #   already classified ranges => increment counters for misses and matches; decrement by dec_function\n",
    "    #   not classified ranges = add IPs\n",
    "    #\n",
    "\n",
    "    dp.search(subnet_dict, f\"{path}/**/ingress\")\n",
    "    match=0\n",
    "    if type(ingress) == list: # handle bundle\n",
    "        # count matches for that ingress'es\n",
    "        bundle_id=len(bundle_dict)+1\n",
    "        bundle_name=\"{}{}{}\".format(ingress[0].split(\".\")[0], bundle_indicator, bundle_id)\n",
    "        # bundle_name+=\",\".join(ingress)\n",
    "        # bundle_name+=\")\"\n",
    "\n",
    "        tmp_dict=defaultdict(int)\n",
    "        for p,v in dp.search(subnet_dict, f\"{path}/**/ingress\", yielded=True):\n",
    "            if v in ingress:\n",
    "                tmp_dict[v] +=1\n",
    "                match +=1\n",
    "\n",
    "        bundle_dict[bundle_name] = tmp_dict\n",
    "\n",
    "        ingress=bundle_name\n",
    "\n",
    "\n",
    "    else: # handle single ingress link\n",
    "\n",
    "        for p,v in dp.search(subnet_dict, f\"{path}/**/ingress\", yielded=True):\n",
    "            if v == ingress: match += 1\n",
    "\n",
    "\n",
    "    sample_count = get_sample_count(path)\n",
    "\n",
    "    last_seen=0\n",
    "    try:   \n",
    "        last_seen = max(dp.search(subnet_dict, f\"{path}/**/last_seen\", yielded=True))[1]\n",
    "    except:\n",
    "        logging.critical(\"last_seen not avaliable: {}\".format(dp.get(subnet_dict, f\"{path}\")))\n",
    "\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "\n",
    "    pr = subnet_dict[ip_version][mask].pop(prange)\n",
    "\n",
    "    logger.debug(f\" remove state for {len(pr)} IPs\")\n",
    "    miss = sample_count-match\n",
    "    dp.new(subnet_dict, f\"{path}/prevalent\", ingress)\n",
    "    # TODO prevalent_\n",
    "    dp.new(subnet_dict, f\"{path}/total\", sample_count)\n",
    "    # dp.new(subnet_dict, f\"{path}/match\", match)\n",
    "    dp.new(subnet_dict, f\"{path}/miss\", miss)\n",
    "    dp.new(subnet_dict, f\"{path}/prevalent_last_seen\", last_seen)\n",
    "\n",
    "\n",
    "    #if DEBUG:\n",
    "    min_samples=__get_min_samples(path)\n",
    "    ratio= match / sample_count\n",
    "    logger.info(f\"        set prevalent ingress: {path} => {ingress}: {ip_version} range {ratio:.3f} {sample_count}/{min_samples} {prange}/{mask} {ingress} | miss: {miss} total: {sample_count}\")\n",
    "\n",
    "\n",
    "# iterates over all ranges that are already classified\n",
    "def is_prevalent_ingress_still_valid():\n",
    "    logger.info(\"  > Prevalent color still valid (s_color >= q)\")\n",
    "\n",
    "    check_list=[]\n",
    "    buffer_dict={}\n",
    "\n",
    "    currently_prevalent_ingresses = dp.search(subnet_dict, \"**/prevalent\", yielded=True)\n",
    "\n",
    "    # prepare inital list\n",
    "    for p,v in currently_prevalent_ingresses:\n",
    "        check_list.append(p)\n",
    "\n",
    "    check_list.sort()\n",
    "    while len(check_list) > 0:\n",
    "        current_prevalent_path = check_list.pop()\n",
    "        \n",
    "\n",
    "        # if we have to handle a sibling where the other one already initiated join\n",
    "        if  buffer_dict.get(current_prevalent_path,False):\n",
    "            buffer_dict.pop(current_prevalent_path)\n",
    "            continue\n",
    "\n",
    "        logger.debug(f\"    checking {current_prevalent_path}\")\n",
    "        ip_version, mask, prange = __convert_range_path_to_single_elems(current_prevalent_path)\n",
    "        current_prevalent = subnet_dict[ip_version][mask][prange]['prevalent']\n",
    "\n",
    "        #current_prevalent= i\n",
    "\n",
    "        new_prevalent = get_prevalent_ingress(current_prevalent_path)\n",
    "\n",
    "\n",
    "        # if new_prevalent is list and current_prevalent is bundle string, we split current_prevalent and compare list\n",
    "        if (current_prevalent == new_prevalent) or ((type(new_prevalent) == list) and (bundle_indicator in current_prevalent) and  (list(bundle_dict.get(current_prevalent).keys()).sort() == sorted(new_prevalent))):\n",
    "            logger.info(\"     YES → join siblings ? (join(s_color ) >= q) \")\n",
    "\n",
    "            r = join_siblings(path=current_prevalent_path, counter_check=False)\n",
    "\n",
    "\n",
    "            # JOIN and add sibling to buffer dict to pop in next iteration; further add new supernet to check_list\n",
    "            if r != None:\n",
    "                joined_supernet, sibling_to_pop = r\n",
    "                buffer_dict[sibling_to_pop] = True\n",
    "                check_list.append(joined_supernet)\n",
    "                check_list.sort()\n",
    "\n",
    "        else:\n",
    "            x = subnet_dict[ip_version][mask].pop(prange)\n",
    "            logger.info(f\"     NO → remove all information for {prange}: {x}\")\n",
    "            \n",
    "            #pop_list.append(p)\n",
    "\n",
    "\n",
    "def split_range(path):\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "\n",
    "    if cidr_max[ip_version] <= mask:\n",
    "        logger.info(\"    max_cidr reached - do nothing\")\n",
    "        return\n",
    "\n",
    "    nw= IPNetwork(f\"{prange}/{mask}\")\n",
    "\n",
    "    # ip_version = str()\n",
    "    #print(f\"nw: {nw}\")\n",
    "    # add range to pytrcia tree and remove supernet\n",
    "    info_txt=f\"          split {prange}/{mask} into\"\n",
    "    for splitted_nw in nw.subnet(mask+1):\n",
    "        #logger.info(f\"     add {splitted_nw}\")\n",
    "        range_lookup_dict[ip_version].insert(str(splitted_nw), str(splitted_nw))\n",
    "        info_txt+=f\" {splitted_nw} and\"\n",
    "    info_txt= info_txt[:-4]\n",
    "    logger.info(info_txt)\n",
    "    # logger.info(f\"     del {nw}\")\n",
    "\n",
    "    range_lookup_dict[ip_version].delete(str(nw))\n",
    "\n",
    "    # now split subnet_dict with all IPs\n",
    "    change_list=[]\n",
    "    for p,v  in dp.search(subnet_dict, f\"{path}/*\", yielded=True): change_list.append((p,v))\n",
    "\n",
    "    logger.debug(\"        #items {}; first 3 elems: {}\".format(len(change_list), change_list[:3]))\n",
    "    subnet_dict[ip_version][mask].pop(prange)\n",
    "    for p,v in change_list:\n",
    "        try:\n",
    "            add_to_subnet(ip= p.split(\"/\")[3], ingress=v.get(\"ingress\"), last_seen=v.get(\"last_seen\"))\n",
    "        except:\n",
    "            logger.warning(f\"         splitting not possible: {p} {v}\")\n",
    "\n",
    "\n",
    "    logger.debug(\"         range_lookup_dict: {}\".format(list(range_lookup_dict[ip_version])))\n",
    "\n",
    "def join_siblings(path, counter_check=True):\n",
    "    logger.info(f\"        join siblings for range {path}\")\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "\n",
    "    ## check if join would be possible\n",
    "\n",
    "    if mask == 0:\n",
    "        logger.info(\"        join siblings not possible - we are at the root of the tree\")\n",
    "        return None\n",
    "\n",
    "    nw = IPNetwork(f\"{prange}/{mask}\")\n",
    "\n",
    "    #what is the potential sibling?\n",
    "    nw_supernet=nw.supernet(mask-1)[0]\n",
    "    supernet_ip=str(nw_supernet).split(\"/\")[0]\n",
    "    supernet_mask=int(str(nw_supernet).split(\"/\")[1])\n",
    "\n",
    "    siblings=list(nw_supernet.subnet(mask))\n",
    "    the_other_one=None\n",
    "    for sibling in siblings:\n",
    "\n",
    "        logger.debug(f\"sibling: {sibling}\")\n",
    "        # if one of both siblings does not exist -> skip joining\n",
    "        if range_lookup_dict[ip_version].get(str(sibling), None) == None: return None\n",
    "\n",
    "        if str(sibling) != f\"{prange}/{mask}\": the_other_one=str(sibling)\n",
    "\n",
    "\n",
    "    # would joining satisfy s_color >= q?\n",
    "    s1=get_prevalent_ingress(__convert_range_string_to_range_path(str(siblings[0])), raw=True)\n",
    "    s2=get_prevalent_ingress(__convert_range_string_to_range_path(str(siblings[1])), raw=True)\n",
    "\n",
    "    if (s1 == None or s2 == None) or (len(s1) == 0 and len(s2) == 0):\n",
    "        logger.warning(\"        both prefixes are empty\")\n",
    "        logger.debug(\"lpm lookup: {}\".format(list(range_lookup_dict[ip_version])))\n",
    "        logger.debug(\"subnet_dict: {} {} {}\".format(subnet_dict.get(ip_version, {}).get(supernet_mask, {}).get(supernet_ip,{})))\n",
    "\n",
    "        # TODO pop s1 and s2 and create supernet\n",
    "        return None\n",
    "    # TODO it can be the case that a bundle is returned here\n",
    "    #   lookup that bundle\n",
    "\n",
    "    # s1 or s2 return\n",
    "    #   a dict with ingress router {\"VIE-SB5.123\" : matching samples, \"miss\" : miss samples}\n",
    "    #   a dict with all routers and there counters\n",
    "    #   a dict with bundle id: {\"VIE-SB5.b_xxxx\" : matching samples, \"miss\" : miss samples}\n",
    "    for sibling_dict in [s1, s2]:\n",
    "        # input {'VIE-SB5.b_123': 123141, 'miss': 32}\n",
    "        for x in  [i for i in sibling_dict.keys() if bundle_indicator in i]:\n",
    "            sibling_dict.update(bundle_dict.get(x))\n",
    "            sibling_dict.pop(x)\n",
    "            #print(\"update \", bundle_dict.get(x))\n",
    "            #print(\"pop \", x)\n",
    "            # now we have a dict with all ingress links separately\n",
    "            # e.g. {'miss': 32, 'VIE-SB5.12': 61000, 'VIE-SB5.10': 61571}\n",
    "\n",
    "\n",
    "    tmp_merged_counter_dict =  {k: s1.get(k, 0) + s2.get(k, 0) for k in set(s1) | set(s2)}\n",
    "    tmp_merged_sample_count = sum(tmp_merged_counter_dict.values())\n",
    "\n",
    "    tmp_cur_prevalent = None\n",
    "    for ingress in tmp_merged_counter_dict:\n",
    "            ratio = tmp_merged_counter_dict.get(ingress) / tmp_merged_sample_count\n",
    "            # logger.info(\"       ratio for {}: {:.2f}\".format(ingress, ratio))\n",
    "            if ratio >= q:\n",
    "                logger.debug(f\" join would set {ingress} as prevalent for {nw_supernet}\")\n",
    "                \n",
    "                tmp_cur_prevalent = ingress\n",
    "\n",
    "    # if join(s_color) >= q  OR join(s_ipcount) < n_cidr-1 => let's join siblings\n",
    "    if (tmp_cur_prevalent != None) or (tmp_merged_sample_count < __get_min_samples(__convert_range_string_to_range_path(str(nw_supernet))) and counter_check):\n",
    "        logger.info(f\" -> join {siblings[0]} and {siblings[1]} to  {nw_supernet}\")\n",
    "        # if both siblings exists -> delete it from range_lookup_dict and add supernet\n",
    "        logger.debug(\"len before: {}\".format(len(subnet_dict[ip_version][supernet_mask][supernet_ip].keys())))\n",
    "\n",
    "        # insert new range to lpm lookup tree\n",
    "        range_lookup_dict[ip_version].insert(str(nw_supernet), str(nw_supernet))\n",
    "\n",
    "        # remove old prefixes from subnet_dict and range_lookup_dict\n",
    "        for sibling in siblings:\n",
    "\n",
    "            # merge subnet trees to supernet\n",
    "            logger.debug(\"{} -> {}\".format(sibling, len(subnet_dict[ip_version][mask][str(sibling).split(\"/\")[0]])))\n",
    "            p= subnet_dict[ip_version][supernet_mask][supernet_ip].update(subnet_dict[ip_version][mask].pop(str(sibling).split(\"/\")[0]))\n",
    "            logger.debug(f\" remove prefix: {p}\")\n",
    "            try:\n",
    "                range_lookup_dict[ip_version].delete(str(sibling))\n",
    "            except:\n",
    "                logger.waning(f\"key {sibling} does not exist\")\n",
    "                logger.debug(\"   {}\".format(range_lookup_dict[ip_version]))\n",
    "                pass\n",
    "        logger.debug(\"len now: {}\".format(len(subnet_dict[ip_version][supernet_mask][supernet_ip].keys())))\n",
    "\n",
    "        #       supernet add to list                          sibling that can be removed\n",
    "        return f\"{ip_version}/{supernet_mask}/{supernet_ip}\", the_other_one\n",
    "        # pop_list=[]\n",
    "        # add_list=[]\n",
    "        # return f\"{ip_version}/{supernet_mask}/{supernet_ip}\", pop_list, add_list\n",
    "\n",
    "    else:\n",
    "        logger.info(\" NO -> do nothing\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_to_subnet(ip, ingress, last_seen):\n",
    "    # cases:\n",
    "    #   1) no prevalent ingress for that range found -> add ip and last_seen timestamp\n",
    "    #   2a) there is one single prevalent link:       -> increment total and increment miss if it is not the correct ingress\n",
    "    #   2b) there is a prevalent bundle:              -> increment total and increment miss in subnet_dict AND increment matches for ingresses in bundle dict\n",
    "\n",
    "    ip_version = 4 if not \":\" in ip else 6\n",
    "\n",
    "    masked_ip = mask_ip(ip)\n",
    "    prange, mask = __split_ip_and_mask(get_corresponding_range(masked_ip))\n",
    "\n",
    "\n",
    "    # get current prev ingress if existing\n",
    "    p_ingress=subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{}).get('prevalent', None)\n",
    "\n",
    "    if p_ingress==None: # 1) no prevalent ingress found for that range\n",
    "        dp.new(subnet_dict, [int(ip_version), int(mask), prange, masked_ip, 'last_seen'], int(last_seen))\n",
    "        dp.new(subnet_dict, [int(ip_version), int(mask), prange, masked_ip, 'ingress'], ingress)\n",
    "\n",
    "    else: # 2) there is already a prevalent link\n",
    "        subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{})['total'] +=1 # increment totals\n",
    "        subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{})['prevalent_last_seen'] = int(last_seen)\n",
    "\n",
    "        if (bundle_indicator in p_ingress) and (ingress in bundle_dict[p_ingress].keys()): # 2b) there is a prevalent bundle:\n",
    "                bundle_dict[p_ingress][ingress] +=1\n",
    "        elif ingress == p_ingress: # 2a) there is one single prevalent link\n",
    "            # do nothing since we already incremented totals\n",
    "            pass\n",
    "        else:\n",
    "            subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{})['miss'] +=1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def __decay_counter(current_ts, path, last_seen, method=\"none\"): # default, linear, stefan\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "    totc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['total']\n",
    "    #matc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['match']\n",
    "    misc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['miss']\n",
    "\n",
    "    age = (current_ts-e) - last_seen\n",
    "\n",
    "    # TODO decay from total\n",
    "\n",
    "    logger.debug(\"total: {} miss: {} age:{}\".format(totc,misc,age) )\n",
    "    reduce = 0\n",
    "    if method == 'default': # ingmar\n",
    "        # my $bucketExpireKeepFraction = 0.9; # all counts get decreased to this fraction every time a bucket is flushed. This fraction decreases for stale buckets\n",
    "        # $cidrIntRef->{ip}{$color}{$ipInt}{u} => $lastUpdate \n",
    "        #\n",
    "        # sub getCleanKeepFactor{\n",
    "        #   (my $expireTime, my $lastUpdate) = @_;\n",
    "        #   my $age = $expireTime - $lastUpdate;\n",
    "        #   return 1 - (($age <= 0) ? $bucketExpireKeepFraction : ($bucketExpireKeepFraction/(int($age/$bucketSize)+1)));\n",
    "        # }\n",
    "        # my $reduce = int($cidrIntRef->{ip}{$color}{$ipInt}{c} * (getCleanKeepFactor($expireTime, $cidrIntRef->{ip}{$color}{$ipInt}{u})));\n",
    "        # $cidrIntRef->{ip}{$color}{$ipInt}{c} -= $reduce;\n",
    "        def get_clean_keep_factor(age):\n",
    "            # age= expire_time - last_update\n",
    "            x = decay_ingmar_bucket_expire_keep_fraction if (age <=0) else decay_ingmar_bucket_expire_keep_fraction / (int(age/t) + 1)\n",
    "            return 1- x  \n",
    "        \n",
    "        totc -= totc * get_clean_keep_factor(age)\n",
    "        misc -= misc * get_clean_keep_factor(age)\n",
    "\n",
    "    elif method == \"stefan\": # 0.1% of min samples for specific mask exponentially increasing by expired time buckets\n",
    "        s = __get_min_samples(path=path, decrement=True)\n",
    "        reduce = int(math.pow(s, (int(age/t)+1 )))\n",
    "        misc -= reduce * (misc / totc)\n",
    "        totc -= reduce\n",
    "\n",
    "\n",
    "    elif method == \"linear\":\n",
    "        if age > e:\n",
    "            reduce = __get_min_samples(path=path, decrement=True)\n",
    "            totc -= 10 #reduce * (matc / (matc + misc))\n",
    "            misc -= 10 #reduce * (misc / (matc + misc))\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    elif method == \"none\":\n",
    "        return\n",
    "\n",
    "    logger.debug(f\"{path} decrement by: {reduce} ({method})\")\n",
    "\n",
    "    subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['total'] = int(totc)\n",
    "    subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['miss'] =  int(misc)\n",
    "\n",
    "# remove all ips older than e seconds\n",
    "def remove_old_ips_from_range(current_ts):\n",
    "    logger.info(f\"  > remove IPs older than {e} seconds\")\n",
    "    pop_list=[]\n",
    "\n",
    "    ## here we have to distinguish between\n",
    "    #       already classified prefixes -> decrement function\n",
    "\n",
    "    for path, ts in dp.search(subnet_dict, \"**/prevalent_last_seen\", yielded=True):\n",
    "        __decay_counter(current_ts=current_ts, path=path, last_seen=ts, method=decay_method)\n",
    "\n",
    "\n",
    "    ##      unclassified prefixies      -> iterate over ip addresses and pop expired ones\n",
    "    for path, ts in dp.search(subnet_dict, \"**/last_seen\",yielded=True):\n",
    "        # print(path, ts)\n",
    "        # age=\n",
    "        if int(ts)  < current_ts - e :\n",
    "            # logger.info(\"remove old ip: {} ({})\".format(path, ts))\n",
    "            pop_list.append(path)\n",
    "\n",
    "    logger.info(\"    removing {} expired IP addresses\".format(len(pop_list)))\n",
    "    # b= len(subnet_dict[\"4\"][\"0\"][\"0.0.0.0\"])\n",
    "    for path in pop_list:\n",
    "        try:\n",
    "            path_elems= path.split(\"/\")\n",
    "            ip_version=int(path_elems[0])\n",
    "            mask=int(path_elems[1])\n",
    "            prange=path_elems[2]\n",
    "            ip=path_elems[3]\n",
    "\n",
    "            #dp.delete(subnet_dict, path.replace(\"/last_seen\", \"\")) # too slow\n",
    "            subnet_dict[ip_version][mask][prange].pop(ip)\n",
    "\n",
    "        except:\n",
    "            logger.warning(\"    ERROR: {} cannot be deleted\".format(path))\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "def dump_to_file(current_ts):\n",
    "    # this should be the output format\n",
    "    # only dump prevalent ingresses here\n",
    "    #\n",
    "    output_file=f\"results/q{q}_c{c[4]}-{c[6]}_cidr_max{cidr_max[4]}-{cidr_max[6]}_t{t}_e{e}_decay{decay_method}\"\n",
    "    os.makedirs(output_file, exist_ok=True)\n",
    "    output_file += f\"/range.{current_ts}.gz\"\n",
    "\n",
    "    logger.info(f\"dump to file: {output_file}\")\n",
    "    with gzip.open(output_file, 'wb') as ipd_writer:\n",
    "        # Needs to be a bytestring in Python 3\n",
    "        with io.TextIOWrapper(ipd_writer, encoding='utf-8') as encode:\n",
    "            #encode.write(\"test\")\n",
    "            for p, i in dp.search(subnet_dict, \"**/prevalent\", yielded=True):\n",
    "            #ipd_writer.write(b\"I'm a log message.\\n\")\n",
    "                #if DEBUG:\n",
    "                logger.debug(\"{} {}\".format(p,i))\n",
    "\n",
    "                ip_version, mask, prange = __convert_range_path_to_single_elems(p)\n",
    "                min_samples=__get_min_samples(p)\n",
    "                p= p.replace(\"/prevalent\", \"\")\n",
    "                #match_samples=int(dp.get(subnet_dict, f\"{p}/match\"))\n",
    "                miss_samples= int(dp.get(subnet_dict, f\"{p}/miss\"))\n",
    "                total_samples= int(dp.get(subnet_dict, f\"{p}/total\"))\n",
    "\n",
    "                ratio= 1-(miss_samples / total_samples)\n",
    "\n",
    "                encode.write(f\"{current_ts}\\t{ip_version}\\trange\\t{ratio:.3f}\\t{total_samples}/{min_samples}\\t{prange}/{mask}\\t{i}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Replacement index 1 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mehneste/ipd_algo/algo.ipynb Zelle 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcurrent ts: \u001b[39m\u001b[39m{\u001b[39;00mcurrent_ts\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m remove_old_ips_from_range(current_ts\u001b[39m=\u001b[39mcurrent_ts)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m is_prevalent_ingress_still_valid() \u001b[39m# smehner -> fixed \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# now go over all already classified ranges        \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m check_list\u001b[39m=\u001b[39m[]\n",
      "\u001b[1;32m/home/mehneste/ipd_algo/algo.ipynb Zelle 5\u001b[0m in \u001b[0;36mis_prevalent_ingress_still_valid\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=333'>334</a>\u001b[0m \u001b[39mif\u001b[39;00m (current_prevalent \u001b[39m==\u001b[39m new_prevalent) \u001b[39mor\u001b[39;00m ((\u001b[39mtype\u001b[39m(new_prevalent) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m (bundle_indicator \u001b[39min\u001b[39;00m current_prevalent) \u001b[39mand\u001b[39;00m  (\u001b[39mlist\u001b[39m(bundle_dict\u001b[39m.\u001b[39mget(current_prevalent)\u001b[39m.\u001b[39mkeys())\u001b[39m.\u001b[39msort() \u001b[39m==\u001b[39m \u001b[39msorted\u001b[39m(new_prevalent))):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=334'>335</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m     YES → join siblings ? (join(s_color ) >= q) \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=336'>337</a>\u001b[0m     r \u001b[39m=\u001b[39m join_siblings(path\u001b[39m=\u001b[39;49mcurrent_prevalent_path, counter_check\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=339'>340</a>\u001b[0m     \u001b[39m# JOIN and add sibling to buffer dict to pop in next iteration; further add new supernet to check_list\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=340'>341</a>\u001b[0m     \u001b[39mif\u001b[39;00m r \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/home/mehneste/ipd_algo/algo.ipynb Zelle 5\u001b[0m in \u001b[0;36mjoin_siblings\u001b[0;34m(path, counter_check)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=426'>427</a>\u001b[0m logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39m        both prefixes are empty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=427'>428</a>\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mlpm lookup: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlist\u001b[39m(range_lookup_dict[ip_version])))\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=428'>429</a>\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39;49m\u001b[39msubnet_dict: \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(subnet_dict\u001b[39m.\u001b[39;49mget(ip_version, {})\u001b[39m.\u001b[39;49mget(supernet_mask, {})\u001b[39m.\u001b[39;49mget(supernet_ip,{})))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=430'>431</a>\u001b[0m \u001b[39m# TODO pop s1 and s2 and create supernet\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=431'>432</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Replacement index 1 out of range for positional args tuple"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Der Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logger.info(\"\\n\\n .............Start.............\")\n",
    "while True:\n",
    "    for current_ts in sorted(netflow_df.ts_end.unique()):\n",
    "        cur_slice = netflow_df.loc[netflow_df.ts_end == current_ts]\n",
    "        \n",
    "        for i in cur_slice.itertuples():\n",
    "            add_to_subnet(ip=i.src_ip, ingress=i.ingress, last_seen=i.ts_end)\n",
    "        logger.info(f\"current ts: {current_ts}\")\n",
    "\n",
    "        remove_old_ips_from_range(current_ts=current_ts)\n",
    "        is_prevalent_ingress_still_valid() # smehner -> fixed \n",
    "        # now go over all already classified ranges        \n",
    "        \n",
    "        \n",
    "        check_list=[]\n",
    "        buffer_dict={}\n",
    "\n",
    "        for current_range in list(range_lookup_dict[4]) + list(range_lookup_dict[6]):\n",
    "            check_list.append( __convert_range_string_to_range_path(current_range))\n",
    "\n",
    "        while len(check_list) > 0:\n",
    "            current_range_path = check_list.pop()\n",
    "\n",
    "            # skip already prevalent ingresses\n",
    "            ip_version, mask, prange = __convert_range_path_to_single_elems(current_range_path)\n",
    "            if subnet_dict[ip_version][mask][prange].get('prevalent', None) != None: continue\n",
    "\n",
    "\n",
    "            if buffer_dict.get(current_range_path, False):\n",
    "                buffer_dict.pop(current_range_path)\n",
    "            else:\n",
    "\n",
    "\n",
    "                logger.info(f\"   current_range: {current_range_path}\")\n",
    "\n",
    "                r = check_if_enough_samples_have_been_collected(current_range_path)\n",
    "                if r == True:\n",
    "                    prevalent_ingress = get_prevalent_ingress(current_range_path) # str or None\n",
    "                    if prevalent_ingress != None:\n",
    "                        logger.info(f\"        YES -> color {current_range_path} with {prevalent_ingress}\")\n",
    "\n",
    "                        set_prevalent_ingress(current_range_path, prevalent_ingress)\n",
    "                        continue\n",
    "                    else:\n",
    "                        logger.info(f\"        NO -> split subnet\")\n",
    "                        split_range(current_range_path)\n",
    "                        continue\n",
    "\n",
    "                elif r == False:\n",
    "                    logger.info(\"      NO -> join siblings\")\n",
    "\n",
    "\n",
    "                    x = join_siblings(current_range_path)\n",
    "                    if x != None:\n",
    "                        joined_supernet, sibling_to_pop = x\n",
    "                        buffer_dict[sibling_to_pop] = True\n",
    "                        check_list.append(joined_supernet)\n",
    "            \n",
    "        \n",
    "                elif r == None:\n",
    "                    logger.info(\"skip this range since there is nothing to do here\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "        if current_ts % bucket_output == 0: # dump every 5 min to file\n",
    "            dump_to_file(current_ts)\n",
    "\n",
    "        logger.debug(\"bundles: \", bundle_dict)\n",
    "        logger.info(\".............Finished.............\\n\\n\")\n",
    "\n",
    "\n",
    "#   Check all ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.get(subnet_dict, \"4/5/48.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_version = 4\n",
    "\n",
    "# for mask in reversed(range(0, cidr_max.get(ip_version),1)):\n",
    "#     k = dp.search(subnet_dict, f\"{ip_version}/{mask}/**/prevalent\", yielded=True)\n",
    "\n",
    "#     for i in k: print(i)\n",
    "k = dp.search(subnet_dict, \"**/prevalent\", yielded=True)\n",
    "\n",
    "# prepare inital list\n",
    "check_list=[]\n",
    "buffer_dict={}\n",
    "for p,v in k:\n",
    "    check_list.append(p)\n",
    "\n",
    "check_list.sort()\n",
    "while len(check_list) > 0:\n",
    "    \n",
    "    r = check_list.pop()\n",
    "    if  check_list.get(r,False):\n",
    "        check_list.pop(r)\n",
    "    else:\n",
    "        x = join_siblings(r)\n",
    "        if x != None:\n",
    "            joined_supernet, sibling_to_pop = x\n",
    "            buffer_dict[sibling_to_pop] = True\n",
    "            check_list.append(joined_supernet)\n",
    "            check_list.sort()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"before: \", list(range_lookup_dict[\"4\"]))\n",
    "# split_range(\"4/0/0.0.0.0\")\n",
    "# print(\"after: \", list(range_lookup_dict[\"4\"]))\n",
    "\n",
    "#print(subnet_dict['4']['3']['128.0.0.0'].keys())\n",
    "print(list(range_lookup_dict['4']))\n",
    "\n",
    "# join_siblings(\"4/3/128.0.0.0\")\n",
    "# print(dp.search(subnet_dict, \"4/2/128.0.0.0\"))\n",
    "# print(dp.search(subnet_dict, \"4/3/128.0.0.0\"))\n",
    "\n",
    "# subnet_dict['4']['2'][\"128.0.0.0.0\"] = subnet_dict['4']['3'].pop(\"128.0.0.0\")\n",
    "# subnet_dict['4']['2'][\"128.0.0.0.0\"] = subnet_dict['4']['3'].pop(\"160.0.0.0\")\n",
    "# subnet_dict['4']['2'][\"128.0.0.0.0\"]\n",
    "\n",
    "#set_prevalent_ingress(\"4/3/128.0.0.0\", \"STEFAN\")\n",
    "\n",
    "dp.search(subnet_dict, \"4/5/16.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "abc\n",
      "abc\n",
      "abc\n",
      "abc\n",
      "abc\n",
      "abc\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "d=\"abc\"\n",
    "t=60\n",
    "\n",
    "params = namedtuple('params', ['d', 't','b',  'e', 'q', 'c4', 'c6', 'cidrmax4', 'cidrmax6', 'decay', 'loglevel'])\n",
    "\n",
    "param_list=[params(d, t, 300, 120, 0.95, 64, 24, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 64, 24, 28, 48, 'linear', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 64, 24, 28, 48, 'stefan', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 32, 12, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 16, 6, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 8, 3, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 4, 1.5, 28, 48, 'default', 'critical'),\n",
    "            ]\n",
    "\n",
    "for i in param_list:\n",
    "    print(i.d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6144\n",
      "131072\n"
     ]
    }
   ],
   "source": [
    "c={}\n",
    "c[6]=24\n",
    "c[4]=64\n",
    "print(__get_min_samples(\"6/48/\"))\n",
    "print(__get_min_samples(\"4/10/\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dask\n",
    "\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# netflow_df= pd.read_csv(netflow_path)\n",
    "netflow_path=\"/data/slow/mehner/netflow100000.csv\" # 00000\n",
    "# netflow_path=\"/data/slow/mehner/netflow.parquet\"\n",
    "# # read number of rows quickly\n",
    "\n",
    "\n",
    "# define a chunksize\n",
    "chunksize = 5000\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# tqdm context\n",
    "ddf = dd.read_csv(netflow_path, blocksize=128*1024*1024, names=[\"src_ip\",\"ts_end\",\"ingress\"])\n",
    "\n",
    "\n",
    "# df = ddf.compute()\n",
    "\n",
    "\n",
    "\n",
    "# length = sum(1 for row in open(netflow_path, 'r'))\n",
    "# with tqdm(total=100000000, desc=\"chunks read: \") as bar:\n",
    "#     # enumerate chunks read without low_memory (it is massive for pandas to precisely assign dtypes)\n",
    "#     for i, chunk in enumerate(pd.read_csv(netflow_path , chunksize=chunksize, low_memory=False)):\n",
    "#         tmp_df = pd.DataFrame()\n",
    "#         # print the chunk number\n",
    "#         # print(i)\n",
    "        \n",
    "#         # append it to df\n",
    "#         #df = df.append(other=chunk)\n",
    "#         df = pd.concat([df, chunk])\n",
    "        \n",
    "#         # update tqdm progress bar\n",
    "#         bar.update(chunksize)\n",
    "# len(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehneste/.local/lib/python3.8/site-packages/dask/dataframe/core.py:4005: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('ts_end', 'int64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_ip</th>\n",
       "      <th>ts_end</th>\n",
       "      <th>ingress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213.81.220.4</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143.244.58.208</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91.127.66.51</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95.103.170.156</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62.178.57.169</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>143.244.58.212</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>152.195.34.152</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>185.133.60.146</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80.110.125.89</td>\n",
       "      <td>1605639600</td>\n",
       "      <td>VIE-SB5.1605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           src_ip      ts_end       ingress\n",
       "0    213.81.220.4  1605639600    VIE-SB5.13\n",
       "1  143.244.58.208  1605639600  VIE-SB5.1605\n",
       "2    91.127.66.51  1605639600    VIE-SB5.13\n",
       "3  95.103.170.156  1605639600    VIE-SB5.13\n",
       "4   62.178.57.169  1605639600  VIE-SB5.1605\n",
       "5  143.244.58.212  1605639600  VIE-SB5.1605\n",
       "6  152.195.34.152  1605639600    VIE-SB5.20\n",
       "7  185.133.60.146  1605639600    VIE-SB5.13\n",
       "8   80.110.125.89  1605639600  VIE-SB5.1605"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 60\n",
    "def bin_it(x): \n",
    "    return int(int(x) / t) * t\n",
    "\n",
    "#l = ddf.ts_end.apply(lambda x: int(int(x) / t) * t, axis=1, meta=ddf)\n",
    "ddf.ts_end = ddf.ts_end.apply(lambda x: int(int(x) / t) * t, meta=('ts_end', 'int64'))) \n",
    "ddf.compute()\n",
    "#ddf.ts_end.map_partitions(bin_it)# ddf.ts_end.apply(bin_it meta=ddf) \n",
    "# ddf.sort_values(by = 'ts_end', inplace=True)\n",
    "\n",
    "ddf.head(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
