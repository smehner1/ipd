{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingress Detection Algorithm\n",
    "\n",
    "```\n",
    "cidr_max = 28   # nax split cidr mask\n",
    "t=60            # seconds\n",
    "e=120           # expire time if IPs\n",
    "q = 0.95        # needed ingress fraction\n",
    "c = 64          # c* sqrt(2^(IPv * max^-cidr)) -> min number of sampled\n",
    "\n",
    "start with no knowledge (only /0 is known) \n",
    "loop\n",
    "    collect IP from Netflow\n",
    "        Filter IPs for ingress\n",
    "        Mask them to cidrmax\n",
    "        Insert IP into corresponding prefix_range\n",
    "\n",
    "Every t seconds \n",
    "    Check all ranges\n",
    "        Remove IPs older than e seconds \n",
    "        Prevalent color still valid (s_color >= q)\n",
    "            YES → join siblings ? (join(s_color ) >= q) \n",
    "                YES → join siblings and check again \n",
    "                NO → do nothing\n",
    "            NO → remove all information\n",
    "\n",
    "        Check if enough samples have been collected (s_ipcount >= n_cidr ) \n",
    "            YES → is a single color prevalent ? (s_color >=q)\n",
    "                YES → color range with link color\n",
    "                NO → split subnet if s_cidr < cidrmax\n",
    "            NO → join siblings ? (join(s_color) >= q or join(s_ipcount) < n_cidr−1)\n",
    "                YES → join siblings and check again \n",
    "                NO → do nothing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2858101/2523315464.py:86: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  netflow_df = pd.read_csv(netflow_path, compression='gzip', header=None, sep=',', quotechar='\"', error_bad_lines=False, names=cols, usecols = ['peer_src_ip', 'in_iface', 'src_ip', 'ts_end'])\n"
     ]
    },
    {
     "ename": "BadGzipFile",
     "evalue": "Not a gzipped file (b'21')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadGzipFile\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/mehneste/ipd_algo/algo.ipynb Zelle 3\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m  ...done\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m# TAG     PEER_SRC_IP  IN IFACE OUT_IFACE SRC_IP          DST_NET        SRC_PORT DST_PORT PROTO  _       _       TS_START        TS_END    PKTS    BYTES\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# 0       194.25.7.141    13      1571    91.127.69.122   31.13.84.4      40730   443     tcp     0       i       1605639641      1605639641 1       121\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m netflow_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(netflow_path, compression\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m, quotechar\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m, error_bad_lines\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, names\u001b[39m=\u001b[39;49mcols, usecols \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mpeer_src_ip\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39min_iface\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msrc_ip\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mts_end\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mread: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(netflow_df)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbithouse/home/mehneste/ipd_algo/algo.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m## pandas pipe  -> https://towardsdatascience.com/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1235\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1235\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1236\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m     kwds\u001b[39m.\u001b[39mpop(key, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m ensure_dtype_objs(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m---> 75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[1;32m     79\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:544\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:734\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1952\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.8/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/lib/python3.8/gzip.py:479\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_member:\n\u001b[1;32m    476\u001b[0m     \u001b[39m# If the _new_member flag is set, we have to\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[39m# jump to the next member, if there is one.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_read()\n\u001b[0;32m--> 479\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_gzip_header():\n\u001b[1;32m    480\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos\n\u001b[1;32m    481\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.8/gzip.py:427\u001b[0m, in \u001b[0;36m_GzipReader._read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\037\u001b[39;00m\u001b[39m\\213\u001b[39;00m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 427\u001b[0m     \u001b[39mraise\u001b[39;00m BadGzipFile(\u001b[39m'\u001b[39m\u001b[39mNot a gzipped file (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m magic)\n\u001b[1;32m    429\u001b[0m (method, flag,\n\u001b[1;32m    430\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_mtime) \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m<BBIxx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_exact(\u001b[39m8\u001b[39m))\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m!=\u001b[39m \u001b[39m8\u001b[39m:\n",
      "\u001b[0;31mBadGzipFile\u001b[0m: Not a gzipped file (b'21')"
     ]
    }
   ],
   "source": [
    "#!pip install netaddr\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import gzip\n",
    "import pytricia\n",
    "import ipaddress\n",
    "from netaddr import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import dpath.util as dp\n",
    "import io\n",
    "import os\n",
    "import logging\n",
    "\n",
    "loglev= logging.INFO # DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "t = 60          # seconds\n",
    "bucket_output = 60\n",
    "e=  120         # 120  # expire time if IPs\n",
    "q = 0.7801        # needed ingress fraction\n",
    "b= 0.05         # allowed delta between bundle load\n",
    "\n",
    "\n",
    "cidr_max = {    # nax split cidr mask\n",
    "    4: 28,\n",
    "    6: 48\n",
    "}\n",
    "c ={            # c* sqrt(2^(IPv * max^-cidr))\n",
    "    4: 0.0064,\n",
    "    6: 0.0024\n",
    "}\n",
    "decay_method='default'\n",
    "\n",
    "decay_ingmar_bucket_expire_keep_fraction=0.9\n",
    "\n",
    "cols=['tag', 'peer_src_ip', 'in_iface', 'out_iface', 'src_ip', 'dst_net', 'src_port', 'dst_port', 'proto', '__', '_', 'ts_start', 'ts_end', 'pkts', 'bytes']\n",
    "bundle_indicator=\".b_\"\n",
    "\n",
    "netflow_path=\"/data/slow/mehner/netflow100000.csv\"\n",
    "ingresslink_file = \"/data/slow/mehner/ipd/ingresslink/1605571200.gz\"                # if we get more netflow, we should adjust the file\n",
    "router_ip_mapping_file=\"/data/slow/mehner/ipd/router_lookup_tables/1605571200.txt\"\n",
    "\n",
    "\n",
    "############################################\n",
    "########### LOGGER CONFIGURATION ###########\n",
    "############################################\n",
    "os.makedirs(\"log\", exist_ok=True)\n",
    "logfile=f\"log/q{q}_c{c[4]}-{c[6]}_cidr_max{cidr_max[4]}-{cidr_max[6]}_t{t}_e{e}_decay{decay_method}.log\"\n",
    "logging.basicConfig(filename=logfile,\n",
    "                    format='%(asctime)s %(levelname)s %(funcName)s %(message)s',\n",
    "                    datefmt='%d-%b-%y %H:%M:%S',\n",
    "                    filemode='w',\n",
    "                    level=loglev)\n",
    "\n",
    "# Creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "###################################################\n",
    "########### ROUTER NAME <--> IP MAPPING ###########\n",
    "###################################################\n",
    "with open(router_ip_mapping_file, 'r') as csv_file:\n",
    "    router_ip_mapping_csv = csv.reader(csv_file, delimiter=' ')\n",
    "    router_ip_lookup_dict = {rows[0]:rows[1] for rows in router_ip_mapping_csv}\n",
    "\n",
    "###################################################\n",
    "###########     INGRESS LINK FILE       ###########\n",
    "###################################################\n",
    "\n",
    "logger.info(f\"load ingresslink file: {ingresslink_file}\")\n",
    "ingresslink_dict= {}\n",
    "with gzip.open(\"{}\".format(ingresslink_file), 'rb') as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8').split(\",\")\n",
    "        router= line[0].replace(\"PEER_SRC_IP=\", \"\")\n",
    "        in_iface= line[1].replace(\"IN_IFACE=\", \"\")\n",
    "\n",
    "        # ingresslink_list.append(\"{}.{}\".format(router, in_iface))\n",
    "        ingresslink_dict[\"{}.{}\".format(router, in_iface)] = True\n",
    "logger.info(\"  ...done\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# TAG     PEER_SRC_IP  IN IFACE OUT_IFACE SRC_IP          DST_NET        SRC_PORT DST_PORT PROTO  _       _       TS_START        TS_END    PKTS    BYTES\n",
    "# 0       194.25.7.141    13      1571    91.127.69.122   31.13.84.4      40730   443     tcp     0       i       1605639641      1605639641 1       121\n",
    "netflow_df = pd.read_csv(netflow_path, compression='gzip', header=None, sep=',', quotechar='\"', error_bad_lines=False, names=cols, usecols = ['peer_src_ip', 'in_iface', 'src_ip', 'ts_end'])\n",
    "logger.debug(\"read: {}\".format(len(netflow_df)))\n",
    "\n",
    "\n",
    "## pandas pipe  -> https://towardsdatascience.com/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0\n",
    "netflow_df['ingress_router'] = netflow_df.peer_src_ip.apply(lambda x: router_ip_lookup_dict.get(x))\n",
    "netflow_df['ingress'] = netflow_df['ingress_router'] + \".\" + netflow_df.in_iface.astype(str)\n",
    "netflow_df.drop(columns=['ingress_router', 'peer_src_ip', 'in_iface'], inplace=True)\n",
    "\n",
    "netflow_df.drop(netflow_df.index[netflow_df['ts_end'] == 'TIMESTAMP_END'], inplace=True)\n",
    "\n",
    "netflow_df['is_ingresslink'] = netflow_df.ingress.apply(lambda x: ingresslink_dict.get(x,False))\n",
    "netflow_df = netflow_df.loc[netflow_df.is_ingresslink]\n",
    "\n",
    "netflow_df.drop(columns=['is_ingresslink'], inplace=True)\n",
    "logger.debug(\"ingress only: \", len(netflow_df))\n",
    "\n",
    "netflow_df['ts_end'] = netflow_df.ts_end.apply(lambda x: int(int(x) / t) * t)\n",
    "netflow_df.sort_values(by = 'ts_end', inplace=True)\n",
    "\n",
    "# mask to cidr max\n",
    "#netflow_df['src_ip'] = netflow_df.src_ip.apply(lambda x: str(ipaddress.ip_network(\"{}/{}\".format(x, cidr_max), strict=False)).split(\"/\")[0])\n",
    "\n",
    "netflow_df = netflow_df.convert_dtypes()\n",
    "\n",
    "netflow_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src_ip     string\n",
       "ts_end      Int64\n",
       "ingress    string\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_df= pd.read_csv(\"/data/slow/mehner/netflow100000.csv\", names= [\"src_ip\",\"ts_end\",\"ingress\"])\n",
    "netflow_df['ts_end'] = netflow_df.ts_end.apply(lambda x: int(int(x) / t) * t) \n",
    "netflow_df.sort_values(by = 'ts_end', inplace=True)\n",
    "netflow_df.head()\n",
    "netflow_df.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################\n",
    "### PROTOTYPING IPDRange Class ###\n",
    "##################################\n",
    "\n",
    "### DICT implementation\n",
    "\n",
    "# if classified range will be in range dict\n",
    "# def __range_atts():\n",
    "    # NOTE last seen will be updated if there is any new IP that belongs to this range\n",
    "    #   if last_seen < 'current now' - e: drop prefix\n",
    "    # return {'last_seen': 0, 'ingress': \"\", 'match' : 0, 'miss' : 0}\n",
    "\n",
    "# if not yet classified range will be in subnet dict - here ip addresses are monitored\n",
    "def __subnet_atts():\n",
    "    return {'last_seen': 0,  'ingress' : \"\"}\n",
    "\n",
    "def __multi_dict(K, type):\n",
    "    if K == 1:\n",
    "        return defaultdict(type)\n",
    "    else:\n",
    "        return defaultdict(lambda: __multi_dict(K-1, type))\n",
    "\n",
    "# something like range_dict[ip_version][range]{last_seen: ... , ingress: ... , match: ... , miss: ... }\n",
    "# range_dict=__multi_dict(2, __range_atts)\n",
    "\n",
    "# something like subnet_dict[ip_version][range][{ip: ... , ingress: ... , last_seen: ... }]\n",
    "#subnet_dict=__multi_dict(3, __subnet_atts)\n",
    "subnet_dict=__multi_dict(4, __subnet_atts) # smehner TESTING\n",
    "\n",
    "# initialization\n",
    "range_lookup_dict = __multi_dict(1, pytricia.PyTricia) #defaultdict(lambda: pytricia.PyTricia())\n",
    "range_lookup_dict[4].insert(\"0.0.0.0/0\", \"0.0.0.0/0\")\n",
    "range_lookup_dict[6].insert(\"::/0\", \"::/0\")\n",
    "\n",
    "bundle_dict={}\n",
    "\n",
    "## lookup in pytricia tree and return corresponding range\n",
    "def get_corresponding_range(ip):\n",
    "    ip_version = 4 if not \":\" in ip else 6\n",
    "    try:\n",
    "        res =range_lookup_dict[ip_version][ip]\n",
    "    except:\n",
    "        logger.warning(f\"key error: {ip}\")\n",
    "        logger.debug(\"  current ranges: {}\".format(list(range_lookup_dict[ip_version])))\n",
    "\n",
    "        res=\"0.0.0.0/0\" if ip_version == 4 else \"::/0\"\n",
    "    # logger.info(\"check corresponding range;  ip: {} ; range: {}\".format(ip_address, res))\n",
    "    return res\n",
    "\n",
    "def mask_ip(ip_address):\n",
    "    ip_version = 6 if \":\" in ip_address else 4\n",
    "    return str(IPNetwork(f\"{ip_address}/{cidr_max[ip_version]}\").network)\n",
    "\n",
    "\n",
    "\n",
    "def __get_min_samples(path, decrement=False):\n",
    "        t = path.split(\"/\")\n",
    "        ip_version = int(t[0])\n",
    "        cidr = int(t[1])\n",
    "\n",
    "        if decrement:\n",
    "            cc= c[ip_version] * 0.001 # take 1% of min_samples as decrement base\n",
    "        else:\n",
    "            cc = c[ip_version]\n",
    "\n",
    "\n",
    "        ipv_max = 32\n",
    "        if ip_version == 6:\n",
    "            ipv_max = 64\n",
    "        min_samples=int(cc * math.sqrt( math.pow(2, (ipv_max - cidr))))\n",
    "\n",
    "        # logger.info(f\"min samples: {min_samples}\")\n",
    "        return min_samples\n",
    "\n",
    "def __split_ip_and_mask(prefix):\n",
    "    # prefix should be in this format 123.123.123.123/12 or 2001:db8:abcd:0012::0/64\n",
    "\n",
    "    ip = prefix.split(\"/\")[0]\n",
    "    mask = prefix.split(\"/\")[1]\n",
    "\n",
    "    return str(ip), int(mask)\n",
    "\n",
    "def __convert_range_string_to_range_path(range_string):\n",
    "    ip_version = 4 if not \":\" in range_string else 6\n",
    "\n",
    "    prange, mask = range_string.split(\"/\")\n",
    "\n",
    "    return f\"{ip_version}/{mask}/{prange}\"\n",
    "\n",
    "def __convert_range_path_to_single_elems(path):\n",
    "    t = path.split(\"/\")\n",
    "    ip_version = int(t[0])\n",
    "    mask = int(t[1])\n",
    "    prange= t[2]\n",
    "    return ip_version, mask, prange\n",
    "\n",
    "def __sort_dict(dict_to_sort):\n",
    "    return {k: dict_to_sort[k] for k in sorted(dict_to_sort, key=dict_to_sort.__getitem__, reverse=True)}\n",
    "\n",
    "\n",
    "def get_sample_count(path):\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "    logger.debug(path)\n",
    "    # matc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}).get('match', -1)\n",
    "    count = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}).get('total', -1)\n",
    "    # misc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}).get('miss', -1)\n",
    "\n",
    "    if count < 0:\n",
    "\n",
    "        # if no prevalent ingress exists, count all items\n",
    "        count= len(subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {}))\n",
    "\n",
    "        if count <=0:\n",
    "            logger.warning(f\" key {path} does not exist\")\n",
    "            return -1\n",
    "\n",
    "    return count\n",
    "\n",
    "def check_if_enough_samples_have_been_collected(prange):\n",
    "    logger.info(f\"  > Check if enough samples have been collected (s_ipcount >= n_cidr ) {prange}\")\n",
    "    sample_count = get_sample_count(prange)\n",
    "    if sample_count < 0: # if -1 -> key error\n",
    "        return None\n",
    "\n",
    "    min_samples=__get_min_samples(prange)\n",
    "    logger.debug(f\"sample_count: {sample_count} || min_samples= {min_samples}\")\n",
    "\n",
    "    if sample_count >= min_samples:\n",
    "        # print(\"    YES → is a single color prevalent ? (s_color >=q)\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# if raw=True: return not prevalent ingress, but dict with counters for all found routers\n",
    "def get_prevalent_ingress(path, raw=False):\n",
    "\n",
    "    cur_prevalent=None\n",
    "    ratio= -1\n",
    "    sample_count=get_sample_count(path)\n",
    "\n",
    "    # calculate prevalent ingress\n",
    "    counter_dict=defaultdict(int)\n",
    "    result_dict={}\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "    p_ingress = subnet_dict.get(ip_version, {}).get(mask, {}).get(prange, {}).get('prevalent', None)\n",
    "    p_total = subnet_dict.get(ip_version, {}).get(mask, {}).get(prange, {}).get('total', None)\n",
    "    p_miss = subnet_dict.get(ip_version, {}).get(mask, {}).get(prange, {}).get('miss', None)\n",
    "\n",
    "    if p_ingress != None and p_total != None: # there is a prevalent ingress yet\n",
    "        if p_total < 1: \n",
    "            pr = subnet_dict[ip_version][mask].pop(prange)\n",
    "            logger.warning(f\"p_total < 1: {path} ingress:{p_ingress} total:{p_total} miss:{p_miss} - pop: {pr}\")\n",
    "            \n",
    "            return None\n",
    "\n",
    "        ratio = 1- (p_miss / p_total)\n",
    "\n",
    "        if raw:\n",
    "            return {p_ingress : (p_total-p_miss), 'miss' : p_miss} # TODO total or matches ?\n",
    "\n",
    "        if ratio >= q:\n",
    "            logger.debug(f\"        already classified: {p_ingress}: ({ratio:.2f})\")\n",
    "            cur_prevalent=p_ingress\n",
    "        else:\n",
    "            logger.warning(f\"        prevalent ingress {p_ingress} for {path} below threshold ({ratio})\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        search_path=\"{}/**/ingress\".format(path)\n",
    "        for p, v in dp.search(subnet_dict, search_path, yielded=True):\n",
    "            counter_dict[v]+=1\n",
    "\n",
    "        # is single ingress prevalent?\n",
    "\n",
    "        for ingress in counter_dict:\n",
    "            ratio = counter_dict.get(ingress) / sample_count\n",
    "            result_dict[ingress] = round(ratio,3)\n",
    "\n",
    "            # logger.info(\"       ratio for {}: {:.2f}\".format(ingress, ratio))\n",
    "            if ratio >= q:  # we found a prevalent ingress point!\n",
    "                cur_prevalent = ingress\n",
    "                if not raw: break\n",
    "\n",
    "        # check for bundles\n",
    "        if cur_prevalent == None: # if we still have not found an ingress, maybe we have a bundle here\n",
    "            bundle_candidates=set()\n",
    "            last_value=None\n",
    "            last_ingress=None\n",
    "            logger.debug(__sort_dict(result_dict))\n",
    "            for ingress in __sort_dict(result_dict):\n",
    "                value = result_dict.get(ingress)\n",
    "                if value < 0.1: break # since it is sorted; otherwise we should use continue here\n",
    "\n",
    "                # first iteration\n",
    "                if last_value == None:\n",
    "                    last_value = value\n",
    "                    last_ingress = ingress\n",
    "                    continue\n",
    "\n",
    "                # 2nd ... nth iteration\n",
    "                if value + b >= last_value:\n",
    "                        # check if there is the same router\n",
    "                        if ingress.split(\".\")[0] == last_ingress.split(\".\")[0]:\n",
    "                            bundle_candidates.add(last_ingress)\n",
    "                            bundle_candidates.add(ingress)\n",
    "\n",
    "                last_value = value\n",
    "                last_ingress = ingress\n",
    "\n",
    "            if len(bundle_candidates) > 0:\n",
    "                logger.debug(f\"bundle candidates: {bundle_candidates}\")\n",
    "                cum_ratio=0\n",
    "                for i in bundle_candidates: cum_ratio += result_dict.get(i)\n",
    "\n",
    "                if cum_ratio >= q:\n",
    "                    # if cum_ratio exceeds q, this will be a bundle\n",
    "                    cur_prevalent=list(bundle_candidates)\n",
    "                    ratio = cum_ratio\n",
    "\n",
    "\n",
    "        if raw:\n",
    "            logger.debug(f\"counter_dict: {counter_dict}\")\n",
    "            return counter_dict\n",
    "\n",
    "\n",
    "    if cur_prevalent == None:\n",
    "        ratio = -1\n",
    "        logger.info(\"        no prevalent ingress found: {}\".format(__sort_dict(result_dict)))\n",
    "\n",
    "    logger.info(\"        prevalent for {}: {} ({:.2f})\".format(path, cur_prevalent, ratio))\n",
    "\n",
    "    return cur_prevalent\n",
    "\n",
    "def set_prevalent_ingress(path, ingress):\n",
    "    # if an ingress is prevalent we set a 'prevalent' attribute for this path\n",
    "    # then we can set the counter for miss and match\n",
    "    # and pop the list with all single ips\n",
    "    # then we need to distinguish between\n",
    "    #   already classified ranges => increment counters for misses and matches; decrement by dec_function\n",
    "    #   not classified ranges = add IPs\n",
    "    #\n",
    "\n",
    "    dp.search(subnet_dict, f\"{path}/**/ingress\")\n",
    "    match=0\n",
    "    if type(ingress) == list: # handle bundle\n",
    "        # count matches for that ingress'es\n",
    "        bundle_id=len(bundle_dict)+1\n",
    "        bundle_name=\"{}{}{}\".format(ingress[0].split(\".\")[0], bundle_indicator, bundle_id)\n",
    "        # bundle_name+=\",\".join(ingress)\n",
    "        # bundle_name+=\")\"\n",
    "\n",
    "        tmp_dict=defaultdict(int)\n",
    "        for p,v in dp.search(subnet_dict, f\"{path}/**/ingress\", yielded=True):\n",
    "            if v in ingress:\n",
    "                tmp_dict[v] +=1\n",
    "                match +=1\n",
    "\n",
    "        bundle_dict[bundle_name] = tmp_dict\n",
    "\n",
    "        ingress=bundle_name\n",
    "\n",
    "\n",
    "    else: # handle single ingress link\n",
    "\n",
    "        for p,v in dp.search(subnet_dict, f\"{path}/**/ingress\", yielded=True):\n",
    "            if v == ingress: match += 1\n",
    "\n",
    "\n",
    "    sample_count = get_sample_count(path)\n",
    "\n",
    "    last_seen=0\n",
    "    try:   \n",
    "        last_seen = max(dp.search(subnet_dict, f\"{path}/**/last_seen\", yielded=True))[1]\n",
    "    except:\n",
    "        logging.critical(\"last_seen not avaliable: {}\".format(dp.get(subnet_dict, f\"{path}\")))\n",
    "\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "\n",
    "    pr = subnet_dict[ip_version][mask].pop(prange)\n",
    "\n",
    "    logger.debug(f\" remove state for {len(pr)} IPs\")\n",
    "    miss = sample_count-match\n",
    "    dp.new(subnet_dict, f\"{path}/prevalent\", ingress)\n",
    "    # TODO prevalent_\n",
    "    dp.new(subnet_dict, f\"{path}/total\", sample_count)\n",
    "    # dp.new(subnet_dict, f\"{path}/match\", match)\n",
    "    dp.new(subnet_dict, f\"{path}/miss\", miss)\n",
    "    dp.new(subnet_dict, f\"{path}/prevalent_last_seen\", last_seen)\n",
    "\n",
    "\n",
    "    #if DEBUG:\n",
    "    min_samples=__get_min_samples(path)\n",
    "    ratio= match / sample_count\n",
    "    logger.info(f\"        set prevalent ingress: {path} => {ingress}: {ip_version} range {ratio:.3f} {sample_count}/{min_samples} {prange}/{mask} {ingress} | miss: {miss} total: {sample_count}\")\n",
    "\n",
    "\n",
    "# iterates over all ranges that are already classified\n",
    "def is_prevalent_ingress_still_valid():\n",
    "    logger.info(\"  > Prevalent color still valid (s_color >= q)\")\n",
    "\n",
    "    check_list=[]\n",
    "    buffer_dict={}\n",
    "\n",
    "    currently_prevalent_ingresses = dp.search(subnet_dict, \"**/prevalent\", yielded=True)\n",
    "\n",
    "    # prepare inital list\n",
    "    for p,v in currently_prevalent_ingresses:\n",
    "        check_list.append(p)\n",
    "\n",
    "    check_list.sort()\n",
    "    while len(check_list) > 0:\n",
    "        current_prevalent_path = check_list.pop()\n",
    "        \n",
    "\n",
    "        # if we have to handle a sibling where the other one already initiated join\n",
    "        if  buffer_dict.get(current_prevalent_path,False):\n",
    "            buffer_dict.pop(current_prevalent_path)\n",
    "            continue\n",
    "\n",
    "        logger.debug(f\"    checking {current_prevalent_path}\")\n",
    "        ip_version, mask, prange = __convert_range_path_to_single_elems(current_prevalent_path)\n",
    "        current_prevalent = subnet_dict[ip_version][mask][prange]['prevalent']\n",
    "\n",
    "        #current_prevalent= i\n",
    "\n",
    "        new_prevalent = get_prevalent_ingress(current_prevalent_path)\n",
    "\n",
    "\n",
    "        # if new_prevalent is list and current_prevalent is bundle string, we split current_prevalent and compare list\n",
    "        if (current_prevalent == new_prevalent) or ((type(new_prevalent) == list) and (bundle_indicator in current_prevalent) and  (list(bundle_dict.get(current_prevalent).keys()).sort() == sorted(new_prevalent))):\n",
    "            logger.info(\"     YES → join siblings ? (join(s_color ) >= q) \")\n",
    "\n",
    "            r = join_siblings(path=current_prevalent_path, counter_check=False)\n",
    "\n",
    "\n",
    "            # JOIN and add sibling to buffer dict to pop in next iteration; further add new supernet to check_list\n",
    "            if r != None:\n",
    "                joined_supernet, sibling_to_pop = r\n",
    "                buffer_dict[sibling_to_pop] = True\n",
    "                check_list.append(joined_supernet)\n",
    "                check_list.sort()\n",
    "\n",
    "        else:\n",
    "            x = subnet_dict[ip_version][mask].pop(prange)\n",
    "            logger.info(f\"     NO → remove all information for {prange}: {x}\")\n",
    "            \n",
    "            #pop_list.append(p)\n",
    "\n",
    "\n",
    "def split_range(path):\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "\n",
    "    if cidr_max[ip_version] <= mask:\n",
    "        logger.info(\"    max_cidr reached - do nothing\")\n",
    "        return\n",
    "\n",
    "    nw= IPNetwork(f\"{prange}/{mask}\")\n",
    "\n",
    "    # ip_version = str()\n",
    "    #print(f\"nw: {nw}\")\n",
    "    # add range to pytrcia tree and remove supernet\n",
    "    info_txt=f\"          split {prange}/{mask} into\"\n",
    "    for splitted_nw in nw.subnet(mask+1):\n",
    "        #logger.info(f\"     add {splitted_nw}\")\n",
    "        range_lookup_dict[ip_version].insert(str(splitted_nw), str(splitted_nw))\n",
    "        info_txt+=f\" {splitted_nw} and\"\n",
    "    info_txt= info_txt[:-4]\n",
    "    logger.info(info_txt)\n",
    "    # logger.info(f\"     del {nw}\")\n",
    "\n",
    "    range_lookup_dict[ip_version].delete(str(nw))\n",
    "\n",
    "    # now split subnet_dict with all IPs\n",
    "    change_list=[]\n",
    "    for p,v  in dp.search(subnet_dict, f\"{path}/*\", yielded=True): change_list.append((p,v))\n",
    "\n",
    "    logger.debug(\"        #items {}; first 3 elems: {}\".format(len(change_list), change_list[:3]))\n",
    "    subnet_dict[ip_version][mask].pop(prange)\n",
    "    for p,v in change_list:\n",
    "        try:\n",
    "            add_to_subnet(ip= p.split(\"/\")[3], ingress=v.get(\"ingress\"), last_seen=v.get(\"last_seen\"))\n",
    "        except:\n",
    "            logger.warning(f\"         splitting not possible: {p} {v}\")\n",
    "\n",
    "\n",
    "    logger.debug(\"         range_lookup_dict: {}\".format(list(range_lookup_dict[ip_version])))\n",
    "\n",
    "def join_siblings(path, counter_check=True):\n",
    "    logger.info(f\"        join siblings for range {path}\")\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "\n",
    "    ## check if join would be possible\n",
    "\n",
    "    if mask == 0:\n",
    "        logger.info(\"        join siblings not possible - we are at the root of the tree\")\n",
    "        return None\n",
    "\n",
    "    nw = IPNetwork(f\"{prange}/{mask}\")\n",
    "\n",
    "    #what is the potential sibling?\n",
    "    nw_supernet=nw.supernet(mask-1)[0]\n",
    "    supernet_ip=str(nw_supernet).split(\"/\")[0]\n",
    "    supernet_mask=int(str(nw_supernet).split(\"/\")[1])\n",
    "\n",
    "    siblings=list(nw_supernet.subnet(mask))\n",
    "    the_other_one=None\n",
    "    for sibling in siblings:\n",
    "\n",
    "        logger.debug(f\"sibling: {sibling}\")\n",
    "        # if one of both siblings does not exist -> skip joining\n",
    "        if range_lookup_dict[ip_version].get(str(sibling), None) == None: return None\n",
    "\n",
    "        if str(sibling) != f\"{prange}/{mask}\": the_other_one=str(sibling)\n",
    "\n",
    "\n",
    "    # would joining satisfy s_color >= q?\n",
    "    s1=get_prevalent_ingress(__convert_range_string_to_range_path(str(siblings[0])), raw=True)\n",
    "    s2=get_prevalent_ingress(__convert_range_string_to_range_path(str(siblings[1])), raw=True)\n",
    "\n",
    "    if (s1 == None or s2 == None) or (len(s1) == 0 and len(s2) == 0):\n",
    "        logger.warning(\"        both prefixes are empty\")\n",
    "        logger.debug(\"lpm lookup: {}\".format(list(range_lookup_dict[ip_version])))\n",
    "        logger.debug(\"subnet_dict: {} {} {}\".format(subnet_dict.get(ip_version, {}).get(supernet_mask, {}).get(supernet_ip,{})))\n",
    "\n",
    "        # TODO pop s1 and s2 and create supernet\n",
    "        return None\n",
    "    # TODO it can be the case that a bundle is returned here\n",
    "    #   lookup that bundle\n",
    "\n",
    "    # s1 or s2 return\n",
    "    #   a dict with ingress router {\"VIE-SB5.123\" : matching samples, \"miss\" : miss samples}\n",
    "    #   a dict with all routers and there counters\n",
    "    #   a dict with bundle id: {\"VIE-SB5.b_xxxx\" : matching samples, \"miss\" : miss samples}\n",
    "    for sibling_dict in [s1, s2]:\n",
    "        # input {'VIE-SB5.b_123': 123141, 'miss': 32}\n",
    "        for x in  [i for i in sibling_dict.keys() if bundle_indicator in i]:\n",
    "            sibling_dict.update(bundle_dict.get(x))\n",
    "            sibling_dict.pop(x)\n",
    "            #print(\"update \", bundle_dict.get(x))\n",
    "            #print(\"pop \", x)\n",
    "            # now we have a dict with all ingress links separately\n",
    "            # e.g. {'miss': 32, 'VIE-SB5.12': 61000, 'VIE-SB5.10': 61571}\n",
    "\n",
    "\n",
    "    tmp_merged_counter_dict =  {k: s1.get(k, 0) + s2.get(k, 0) for k in set(s1) | set(s2)}\n",
    "    tmp_merged_sample_count = sum(tmp_merged_counter_dict.values())\n",
    "\n",
    "    tmp_cur_prevalent = None\n",
    "    for ingress in tmp_merged_counter_dict:\n",
    "            ratio = tmp_merged_counter_dict.get(ingress) / tmp_merged_sample_count\n",
    "            # logger.info(\"       ratio for {}: {:.2f}\".format(ingress, ratio))\n",
    "            if ratio >= q:\n",
    "                logger.debug(f\" join would set {ingress} as prevalent for {nw_supernet}\")\n",
    "                \n",
    "                tmp_cur_prevalent = ingress\n",
    "\n",
    "    # if join(s_color) >= q  OR join(s_ipcount) < n_cidr-1 => let's join siblings\n",
    "    if (tmp_cur_prevalent != None) or (tmp_merged_sample_count < __get_min_samples(__convert_range_string_to_range_path(str(nw_supernet))) and counter_check):\n",
    "        logger.info(f\" -> join {siblings[0]} and {siblings[1]} to  {nw_supernet}\")\n",
    "        # if both siblings exists -> delete it from range_lookup_dict and add supernet\n",
    "        logger.debug(\"len before: {}\".format(len(subnet_dict[ip_version][supernet_mask][supernet_ip].keys())))\n",
    "\n",
    "        # insert new range to lpm lookup tree\n",
    "        range_lookup_dict[ip_version].insert(str(nw_supernet), str(nw_supernet))\n",
    "\n",
    "        # remove old prefixes from subnet_dict and range_lookup_dict\n",
    "        for sibling in siblings:\n",
    "\n",
    "            # merge subnet trees to supernet\n",
    "            logger.debug(\"{} -> {}\".format(sibling, len(subnet_dict[ip_version][mask][str(sibling).split(\"/\")[0]])))\n",
    "            p= subnet_dict[ip_version][supernet_mask][supernet_ip].update(subnet_dict[ip_version][mask].pop(str(sibling).split(\"/\")[0]))\n",
    "            logger.debug(f\" remove prefix: {p}\")\n",
    "            try:\n",
    "                range_lookup_dict[ip_version].delete(str(sibling))\n",
    "            except:\n",
    "                logger.waning(f\"key {sibling} does not exist\")\n",
    "                logger.debug(\"   {}\".format(range_lookup_dict[ip_version]))\n",
    "                pass\n",
    "        logger.debug(\"len now: {}\".format(len(subnet_dict[ip_version][supernet_mask][supernet_ip].keys())))\n",
    "\n",
    "        #       supernet add to list                          sibling that can be removed\n",
    "        return f\"{ip_version}/{supernet_mask}/{supernet_ip}\", the_other_one\n",
    "        # pop_list=[]\n",
    "        # add_list=[]\n",
    "        # return f\"{ip_version}/{supernet_mask}/{supernet_ip}\", pop_list, add_list\n",
    "\n",
    "    else:\n",
    "        logger.info(\" NO -> do nothing\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_to_subnet(ip, ingress, last_seen):\n",
    "    # cases:\n",
    "    #   1) no prevalent ingress for that range found -> add ip and last_seen timestamp\n",
    "    #   2a) there is one single prevalent link:       -> increment total and increment miss if it is not the correct ingress\n",
    "    #   2b) there is a prevalent bundle:              -> increment total and increment miss in subnet_dict AND increment matches for ingresses in bundle dict\n",
    "\n",
    "    ip_version = 4 if not \":\" in ip else 6\n",
    "\n",
    "    masked_ip = mask_ip(ip)\n",
    "    prange, mask = __split_ip_and_mask(get_corresponding_range(masked_ip))\n",
    "\n",
    "\n",
    "    # get current prev ingress if existing\n",
    "    p_ingress=subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{}).get('prevalent', None)\n",
    "\n",
    "    if p_ingress==None: # 1) no prevalent ingress found for that range\n",
    "        dp.new(subnet_dict, [int(ip_version), int(mask), prange, masked_ip, 'last_seen'], int(last_seen))\n",
    "        dp.new(subnet_dict, [int(ip_version), int(mask), prange, masked_ip, 'ingress'], ingress)\n",
    "\n",
    "    else: # 2) there is already a prevalent link\n",
    "        subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{})['total'] +=1 # increment totals\n",
    "        subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{})['prevalent_last_seen'] = int(last_seen)\n",
    "\n",
    "        if (bundle_indicator in p_ingress) and (ingress in bundle_dict[p_ingress].keys()): # 2b) there is a prevalent bundle:\n",
    "                bundle_dict[p_ingress][ingress] +=1\n",
    "        elif ingress == p_ingress: # 2a) there is one single prevalent link\n",
    "            # do nothing since we already incremented totals\n",
    "            pass\n",
    "        else:\n",
    "            subnet_dict.get(ip_version, {}).get(mask,{}).get(prange,{})['miss'] +=1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def __decay_counter(current_ts, path, last_seen, method=\"none\"): # default, linear, stefan\n",
    "\n",
    "    ip_version, mask, prange = __convert_range_path_to_single_elems(path)\n",
    "    totc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['total']\n",
    "    #matc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['match']\n",
    "    misc = subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['miss']\n",
    "\n",
    "    age = (current_ts-e) - last_seen\n",
    "\n",
    "    # TODO decay from total\n",
    "\n",
    "    logger.debug(\"total: {} miss: {} age:{}\".format(totc,misc,age) )\n",
    "    reduce = 0\n",
    "    if method == 'default': # ingmar\n",
    "        # my $bucketExpireKeepFraction = 0.9; # all counts get decreased to this fraction every time a bucket is flushed. This fraction decreases for stale buckets\n",
    "        # $cidrIntRef->{ip}{$color}{$ipInt}{u} => $lastUpdate \n",
    "        #\n",
    "        # sub getCleanKeepFactor{\n",
    "        #   (my $expireTime, my $lastUpdate) = @_;\n",
    "        #   my $age = $expireTime - $lastUpdate;\n",
    "        #   return 1 - (($age <= 0) ? $bucketExpireKeepFraction : ($bucketExpireKeepFraction/(int($age/$bucketSize)+1)));\n",
    "        # }\n",
    "        # my $reduce = int($cidrIntRef->{ip}{$color}{$ipInt}{c} * (getCleanKeepFactor($expireTime, $cidrIntRef->{ip}{$color}{$ipInt}{u})));\n",
    "        # $cidrIntRef->{ip}{$color}{$ipInt}{c} -= $reduce;\n",
    "        def get_clean_keep_factor(age):\n",
    "            # age= expire_time - last_update\n",
    "            x = decay_ingmar_bucket_expire_keep_fraction if (age <=0) else decay_ingmar_bucket_expire_keep_fraction / (int(age/t) + 1)\n",
    "            return 1- x  \n",
    "        \n",
    "        totc -= totc * get_clean_keep_factor(age)\n",
    "        misc -= misc * get_clean_keep_factor(age)\n",
    "\n",
    "    elif method == \"stefan\": # 0.1% of min samples for specific mask exponentially increasing by expired time buckets\n",
    "        s = __get_min_samples(path=path, decrement=True)\n",
    "        reduce = int(math.pow(s, (int(age/t)+1 )))\n",
    "        misc -= reduce * (misc / totc)\n",
    "        totc -= reduce\n",
    "\n",
    "\n",
    "    elif method == \"linear\":\n",
    "        if age > e:\n",
    "            reduce = __get_min_samples(path=path, decrement=True)\n",
    "            totc -= 10 #reduce * (matc / (matc + misc))\n",
    "            misc -= 10 #reduce * (misc / (matc + misc))\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    elif method == \"none\":\n",
    "        return\n",
    "\n",
    "    logger.debug(f\"{path} decrement by: {reduce} ({method})\")\n",
    "\n",
    "    subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['total'] = int(totc)\n",
    "    subnet_dict.get(ip_version,{}).get(mask,{}).get(prange, {})['miss'] =  int(misc)\n",
    "\n",
    "# remove all ips older than e seconds\n",
    "def remove_old_ips_from_range(current_ts):\n",
    "    logger.info(f\"  > remove IPs older than {e} seconds\")\n",
    "    pop_list=[]\n",
    "\n",
    "    ## here we have to distinguish between\n",
    "    #       already classified prefixes -> decrement function\n",
    "\n",
    "    for path, ts in dp.search(subnet_dict, \"**/prevalent_last_seen\", yielded=True):\n",
    "        __decay_counter(current_ts=current_ts, path=path, last_seen=ts, method=decay_method)\n",
    "\n",
    "\n",
    "    ##      unclassified prefixies      -> iterate over ip addresses and pop expired ones\n",
    "    for path, ts in dp.search(subnet_dict, \"**/last_seen\",yielded=True):\n",
    "        # print(path, ts)\n",
    "        # age=\n",
    "        if int(ts)  < current_ts - e :\n",
    "            # logger.info(\"remove old ip: {} ({})\".format(path, ts))\n",
    "            pop_list.append(path)\n",
    "\n",
    "    logger.info(\"    removing {} expired IP addresses\".format(len(pop_list)))\n",
    "    # b= len(subnet_dict[\"4\"][\"0\"][\"0.0.0.0\"])\n",
    "    for path in pop_list:\n",
    "        try:\n",
    "            path_elems= path.split(\"/\")\n",
    "            ip_version=int(path_elems[0])\n",
    "            mask=int(path_elems[1])\n",
    "            prange=path_elems[2]\n",
    "            ip=path_elems[3]\n",
    "\n",
    "            #dp.delete(subnet_dict, path.replace(\"/last_seen\", \"\")) # too slow\n",
    "            subnet_dict[ip_version][mask][prange].pop(ip)\n",
    "\n",
    "        except:\n",
    "            logger.warning(\"    ERROR: {} cannot be deleted\".format(path))\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "def dump_to_file(current_ts):\n",
    "    # this should be the output format\n",
    "    # only dump prevalent ingresses here\n",
    "    #\n",
    "    output_file=f\"results/q{q}_c{c[4]}-{c[6]}_cidr_max{cidr_max[4]}-{cidr_max[6]}_t{t}_e{e}_decay{decay_method}\"\n",
    "    os.makedirs(output_file, exist_ok=True)\n",
    "    output_file += f\"/range.{current_ts}.gz\"\n",
    "\n",
    "    logger.info(f\"dump to file: {output_file}\")\n",
    "    with gzip.open(output_file, 'wb') as ipd_writer:\n",
    "        # Needs to be a bytestring in Python 3\n",
    "        with io.TextIOWrapper(ipd_writer, encoding='utf-8') as encode:\n",
    "            #encode.write(\"test\")\n",
    "            for p, i in dp.search(subnet_dict, \"**/prevalent\", yielded=True):\n",
    "            #ipd_writer.write(b\"I'm a log message.\\n\")\n",
    "                #if DEBUG:\n",
    "                logger.debug(\"{} {}\".format(p,i))\n",
    "\n",
    "                ip_version, mask, prange = __convert_range_path_to_single_elems(p)\n",
    "                min_samples=__get_min_samples(p)\n",
    "                p= p.replace(\"/prevalent\", \"\")\n",
    "                #match_samples=int(dp.get(subnet_dict, f\"{p}/match\"))\n",
    "                miss_samples= int(dp.get(subnet_dict, f\"{p}/miss\"))\n",
    "                total_samples= int(dp.get(subnet_dict, f\"{p}/total\"))\n",
    "\n",
    "                ratio= 1-(miss_samples / total_samples)\n",
    "\n",
    "                encode.write(f\"{current_ts}\\t{ip_version}\\trange\\t{ratio:.3f}\\t{total_samples}/{min_samples}\\t{prange}/{mask}\\t{i}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " .............Start.............\n",
      "current ts: 1605639480\n",
      "current ts: 1605639540\n",
      "current ts: 1605639600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\\n .............Start.............\")\n",
    "for current_ts in sorted(netflow_df.ts_end.unique()):\n",
    "    cur_slice = netflow_df.loc[netflow_df.ts_end == current_ts]\n",
    "    \n",
    "    for i in cur_slice.itertuples():\n",
    "        add_to_subnet(ip=i.src_ip, ingress=i.ingress, last_seen=i.ts_end)\n",
    "    print(f\"current ts: {current_ts}\")\n",
    "\n",
    "    remove_old_ips_from_range(current_ts=current_ts)\n",
    "    is_prevalent_ingress_still_valid() # smehner -> fixed \n",
    "    # now go over all already classified ranges        \n",
    "    \n",
    "    \n",
    "    check_list=[]\n",
    "    buffer_dict={}\n",
    "\n",
    "    for current_range in list(range_lookup_dict[4]) + list(range_lookup_dict[6]):\n",
    "\n",
    "\n",
    "        check_list.append( __convert_range_string_to_range_path(current_range))\n",
    "\n",
    "        while len(check_list) > 0:\n",
    "            current_range_path = check_list.pop()\n",
    "\n",
    "            # skip already prevalent ingresses\n",
    "            ip_version, mask, prange = __convert_range_path_to_single_elems(current_range_path)\n",
    "            if subnet_dict[ip_version][mask][prange].get('prevalent', None) != None: continue\n",
    "\n",
    "\n",
    "            if buffer_dict.get(current_range_path, False):\n",
    "                buffer_dict.pop(current_range_path)\n",
    "            else:\n",
    "\n",
    "\n",
    "                logger.info(f\"   current_range: {current_range_path}\")\n",
    "\n",
    "                r = check_if_enough_samples_have_been_collected(current_range_path)\n",
    "                if r == True:\n",
    "                    prevalent_ingress = get_prevalent_ingress(current_range_path) # str or None\n",
    "                    \n",
    "                    if prevalent_ingress != None:\n",
    "                        logger.info(f\"        YES -> color {current_range_path} with {prevalent_ingress}\")\n",
    "\n",
    "                        set_prevalent_ingress(current_range_path, prevalent_ingress)\n",
    "                        continue\n",
    "                    else:\n",
    "                        logger.info(f\"        NO -> split subnet\")\n",
    "                        split_range(current_range_path)\n",
    "                        continue\n",
    "\n",
    "                elif r == False:\n",
    "                    logger.info(\"      NO -> join siblings\")\n",
    "\n",
    "\n",
    "                    x = join_siblings(current_range_path)\n",
    "                    if x != None:\n",
    "                        joined_supernet, sibling_to_pop = x\n",
    "                        buffer_dict[sibling_to_pop] = True\n",
    "                        check_list.append(joined_supernet)\n",
    "            \n",
    "        \n",
    "                elif r == None:\n",
    "                    logger.info(\"skip this range since there is nothing to do here\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "        if current_ts % bucket_output == 0: # dump every 5 min to file\n",
    "            dump_to_file(current_ts)\n",
    "\n",
    "        logger.debug(\"bundles: \", bundle_dict)\n",
    "        logger.info(\".............Finished.............\\n\\n\")\n",
    "\n",
    "\n",
    "#   Check all ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prevalent_ingress(\"4/0/0.0.0.0\", raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_version = 4\n",
    "\n",
    "# for mask in reversed(range(0, cidr_max.get(ip_version),1)):\n",
    "#     k = dp.search(subnet_dict, f\"{ip_version}/{mask}/**/prevalent\", yielded=True)\n",
    "\n",
    "#     for i in k: print(i)\n",
    "k = dp.search(subnet_dict, \"**/prevalent\", yielded=True)\n",
    "\n",
    "# prepare inital list\n",
    "check_list=[]\n",
    "buffer_dict={}\n",
    "for p,v in k:\n",
    "    check_list.append(p)\n",
    "\n",
    "check_list.sort()\n",
    "while len(check_list) > 0:\n",
    "    \n",
    "    r = check_list.pop()\n",
    "    if  check_list.get(r,False):\n",
    "        check_list.pop(r)\n",
    "    else:\n",
    "        x = join_siblings(r)\n",
    "        if x != None:\n",
    "            joined_supernet, sibling_to_pop = x\n",
    "            buffer_dict[sibling_to_pop] = True\n",
    "            check_list.append(joined_supernet)\n",
    "            check_list.sort()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"before: \", list(range_lookup_dict[\"4\"]))\n",
    "# split_range(\"4/0/0.0.0.0\")\n",
    "# print(\"after: \", list(range_lookup_dict[\"4\"]))\n",
    "\n",
    "#print(subnet_dict['4']['3']['128.0.0.0'].keys())\n",
    "print(list(range_lookup_dict['4']))\n",
    "\n",
    "# join_siblings(\"4/3/128.0.0.0\")\n",
    "# print(dp.search(subnet_dict, \"4/2/128.0.0.0\"))\n",
    "# print(dp.search(subnet_dict, \"4/3/128.0.0.0\"))\n",
    "\n",
    "# subnet_dict['4']['2'][\"128.0.0.0.0\"] = subnet_dict['4']['3'].pop(\"128.0.0.0\")\n",
    "# subnet_dict['4']['2'][\"128.0.0.0.0\"] = subnet_dict['4']['3'].pop(\"160.0.0.0\")\n",
    "# subnet_dict['4']['2'][\"128.0.0.0.0\"]\n",
    "\n",
    "#set_prevalent_ingress(\"4/3/128.0.0.0\", \"STEFAN\")\n",
    "\n",
    "dp.search(subnet_dict, \"4/5/16.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "d=\"abc\"\n",
    "t=60\n",
    "\n",
    "params = namedtuple('params', ['d', 't','b',  'e', 'q', 'c4', 'c6', 'cidrmax4', 'cidrmax6', 'decay', 'loglevel'])\n",
    "\n",
    "param_list=[params(d, t, 300, 120, 0.95, 64, 24, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 64, 24, 28, 48, 'linear', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 64, 24, 28, 48, 'stefan', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 32, 12, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 16, 6, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 8, 3, 28, 48, 'default', 'critical'),\n",
    "            params(d, t, 300, 120, 0.95, 4, 1.5, 28, 48, 'default', 'critical'),\n",
    "            ]\n",
    "\n",
    "for i in param_list:\n",
    "    print(i.d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c={}\n",
    "c[6]=24\n",
    "c[4]=64\n",
    "print(__get_min_samples(\"6/48/\"))\n",
    "print(__get_min_samples(\"4/10/\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dask\n",
    "\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# netflow_df= pd.read_csv(netflow_path)\n",
    "netflow_path=\"/data/slow/mehner/netflow.csv\" # 00000\n",
    "# netflow_path=\"/data/slow/mehner/netflow.parquet\"\n",
    "# # read number of rows quickly\n",
    "\n",
    "\n",
    "# define a chunksize\n",
    "chunksize = 5000\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# tqdm context\n",
    "# ddf = dd.read_csv(netflow_path, blocksize=128*1024*1024, names=[\"src_ip\",\"ts_end\",\"ingress\"])\n",
    "\n",
    "\n",
    "# df = ddf.compute()\n",
    "\n",
    "\n",
    "\n",
    "length = sum(1 for row in open(netflow_path, 'r'))\n",
    "# with tqdm(total=100000000, desc=\"chunks read: \") as bar:\n",
    "#     # enumerate chunks read without low_memory (it is massive for pandas to precisely assign dtypes)\n",
    "#     for i, chunk in enumerate(pd.read_csv(netflow_path , chunksize=chunksize, low_memory=False)):\n",
    "#         tmp_df = pd.DataFrame()\n",
    "#         # print the chunk number\n",
    "#         # print(i)\n",
    "        \n",
    "#         # append it to df\n",
    "#         #df = df.append(other=chunk)\n",
    "#         df = pd.concat([df, chunk])\n",
    "        \n",
    "#         # update tqdm progress bar\n",
    "#         bar.update(chunksize)\n",
    "# len(df)\n",
    "\n",
    "print(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 60\n",
    "def bin_it(x): \n",
    "    return int(int(x) / t) * t\n",
    "\n",
    "#l = ddf.ts_end.apply(lambda x: int(int(x) / t) * t, axis=1, meta=ddf)\n",
    "ddf.ts_end = ddf.ts_end.apply(lambda x: int(int(x) / t) * t, meta=('ts_end', 'int64'))\n",
    "ddf.compute()\n",
    "#ddf.ts_end.map_partitions(bin_it)# ddf.ts_end.apply(bin_it meta=ddf) \n",
    "# ddf.sort_values(by = 'ts_end', inplace=True)\n",
    "\n",
    "\n",
    "for current_ts in sorted(ddf.ts_end.unique()):\n",
    "        cur_slice = ddf.loc[ddf.ts_end == current_ts]\n",
    "        print(len(cur_slice))\n",
    "ddf.head(9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_path_parquet=\"/data/slow/mehner/netflow.parquet\" # 00000\n",
    "pd.read_parquet(netflow_path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_path=\"/data/slow/mehner/netflow100000.csv\" # 00000\n",
    "df1 = pd.read_csv(netflow_path, header=None, names=[\"src_ip\",\"ts_end\",\"ingress\"])\n",
    "df2 = pd.read_csv(netflow_path, header=None, names=[\"src_ip\",\"ts_end\",\"ingress\"])\n",
    "\n",
    "df1 = pd.concat([df1, df2])\n",
    "df1 = pd.concat([df1, df2])\n",
    "df1 = pd.concat([df1, df2])\n",
    "df1 = pd.concat([df1, df2])\n",
    "df1 = pd.concat([df1, df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get factor screening input\n",
    "\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "\n",
    "\n",
    "params = namedtuple('params', ['t','b',  'e', 'q', 'c4', 'c6', 'cidrmax4', 'cidrmax6', 'decay', 'loglevel'])\n",
    "\n",
    "t= [10, 30, 60, 120]\n",
    "b= [0.05]\n",
    "e = [30, 120, 300]\n",
    "q = [0.99, 0.95, 0.85, 0.75, 0.65, 0.55]\n",
    "c4 = [64, 32, 16, 8, 4, 2, 1]\n",
    "c6 = [24, 12,  6, 3 ,1.5, 0.75, 0.5]\n",
    "decay=['default', 'linear', 'stefan']\n",
    "\n",
    "list(itertools.product(e, q, decay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generators playground\n",
    "\n",
    "import gzip\n",
    "import csv\n",
    "netflow_path=\"/data/slow/mehner/ipd/dummy_nf_sorted.gz\"\n",
    "\n",
    "ingresslink_file = \"/data/slow/mehner/ipd/ingresslink/1605571200.gz\"                # if we get more netflow, we should adjust the file \n",
    "router_ip_mapping_file=\"/data/slow/mehner/ipd/router_lookup_tables/1605571200.txt\"\n",
    "\n",
    "t=60\n",
    "\n",
    "###################################################\n",
    "########### ROUTER NAME <--> IP MAPPING ###########\n",
    "###################################################\n",
    "with open(router_ip_mapping_file, 'r') as csv_file:\n",
    "    router_ip_mapping_csv = csv.reader(csv_file, delimiter=' ')\n",
    "    router_ip_lookup_dict = {rows[0]:rows[1] for rows in router_ip_mapping_csv}\n",
    "\n",
    "###################################################\n",
    "###########     INGRESS LINK FILE       ###########\n",
    "###################################################\n",
    "\n",
    "print(\"> load ingresslink file\")\n",
    "\n",
    "ingresslink_dict= {}\n",
    "with gzip.open(\"{}\".format(ingresslink_file), 'rb') as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8').split(\",\")\n",
    "        router= line[0].replace(\"PEER_SRC_IP=\", \"\")\n",
    "        in_iface= line[1].replace(\"IN_IFACE=\", \"\")\n",
    "        \n",
    "        # ingresslink_list.append(\"{}.{}\".format(router, in_iface))\n",
    "        ingresslink_dict[\"{}.{}\".format(router, in_iface)] = True\n",
    "print(\"  ...done\\n\")\n",
    "\n",
    "\n",
    "def read_next_netflow(d):\n",
    "    # returns next netflow from file\n",
    "    # preprocessing will be done here\n",
    "        # for i in glob ...\n",
    "    count=0\n",
    "    with gzip.open(d, 'rb') as f:\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.decode('utf-8').split(\",\")\n",
    "            router_name = router_ip_lookup_dict.get(line[1])\n",
    "            in_iface = line[2]\n",
    "        \n",
    "            if line[-3] == \"TIMESTAMP_END\": continue\n",
    "            if not ingresslink_dict.get(\"{}.{}\".format(router_name,in_iface), False): continue\n",
    "\n",
    "            src_ip = line[4]    \n",
    "            cur_ts = str(int(int(line[-3]) / t) * t)\n",
    "            if count > 5: return\n",
    "            count+=1\n",
    "            yield line\n",
    "\n",
    "hans = read_next_netflow(netflow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def __multi_dict(K, type):\n",
    "    if K == 1:\n",
    "        return defaultdict(type)\n",
    "    else:\n",
    "        return defaultdict(lambda: __multi_dict(K-1, type))\n",
    "\n",
    "t = __multi_dict(2, tuple)\n",
    "\n",
    "t[4][\"123.123.123.123\"] = (\"0.0.0.2\", 2)\n",
    "t[4][\"123.123.123.124\"] = (\"0.0.0.3\", 3)\n",
    "t[4][\"123.123.123.125\"] = (\"0.0.0.4\", 4)\n",
    "t[4][\"123.123.123.126\"] = (\"0.0.0.5\", 4)\n",
    "t[4][\"123.123.123.127\"] = (\"0.0.0.6\", 5)\n",
    "t[4][\"123.123.123.128\"] = (\"0.0.0.7\", 6)\n",
    "t[6][\"123.123..127\"] = (\"0.0.0.6\", 5)\n",
    "t[6][\"123.123.123.128\"] = (\"0.0.0.7\", 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.clear()\n",
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
